{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qOvyEE4dijNL",
        "m5XsKVtHONnn",
        "-WOUWJODnSnz",
        "Cu-L9yyqW_gE",
        "fO1W0Jr7CqhG",
        "-oiF5QdykAec",
        "4gNGAA2Sk6CM",
        "CHB25jzuN1m1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "qOvyEE4dijNL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNB_zgXniYF7",
        "outputId": "4760f78e-eaa4-4736-b612-a4e04f2f59d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GB5RRtoii6Fi",
        "outputId": "a95f1120-d406-4309-e668-78000d4dee69"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-09 23:51:47.666237: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-09 23:51:48.799295: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.5.2                         \n",
            "Location         /usr/local/lib/python3.10/dist-packages/spacy\n",
            "Platform         Linux-5.10.147+-x86_64-with-glibc2.31\n",
            "Python version   3.10.11                       \n",
            "Pipelines        en_core_web_sm (3.5.0)        \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "print(spacy.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeQjVhQ-i6oy",
        "outputId": "f81e20e6-d13d-4ff4-fb63-1bf2aea0afe7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnUOFsaFiipb",
        "outputId": "0492ea6a-2bec-4458-e5fd-6cd7064cb1de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-09 23:52:10.745230: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.2)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanfordcorenlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPwrZQYJj47q",
        "outputId": "37b853f3-b4b2-4549-cd52-ca19ec6af6f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stanfordcorenlp\n",
            "  Downloading stanfordcorenlp-3.9.1.1-py2.py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stanfordcorenlp) (5.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanfordcorenlp) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordcorenlp) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordcorenlp) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordcorenlp) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanfordcorenlp) (3.4)\n",
            "Installing collected packages: stanfordcorenlp\n",
            "Successfully installed stanfordcorenlp-3.9.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install language-tool-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y08whO3EG4Jc",
        "outputId": "5abe109d-884a-45e3-ce0c-7ebc7746861a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (3.4)\n",
            "Installing collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "m5XsKVtHONnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "import uuid\n",
        "import os\n",
        "import json\n",
        "from stanfordcorenlp import StanfordCoreNLP"
      ],
      "metadata": {
        "id": "Sv5CiXygOM5x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Stanford CoreNLP model\n",
        "!wget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
        "!unzip stanford-corenlp-full-2018-10-05.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwX8559VqyLX",
        "outputId": "9fb3b674-a2aa-4bc9-e49f-bf41bb8838cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-09 23:53:07--  http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip [following]\n",
            "--2023-05-09 23:53:07--  https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-full-2018-10-05.zip [following]\n",
            "--2023-05-09 23:53:08--  https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-full-2018-10-05.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 393239982 (375M) [application/zip]\n",
            "Saving to: ‘stanford-corenlp-full-2018-10-05.zip’\n",
            "\n",
            "stanford-corenlp-fu 100%[===================>] 375.02M  5.12MB/s    in 71s     \n",
            "\n",
            "2023-05-09 23:54:19 (5.31 MB/s) - ‘stanford-corenlp-full-2018-10-05.zip’ saved [393239982/393239982]\n",
            "\n",
            "Archive:  stanford-corenlp-full-2018-10-05.zip\n",
            "   creating: stanford-corenlp-full-2018-10-05/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-core-2.3.0.1-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/xom-1.2.10-src.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/CoreNLP-to-HTML.xsl  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/README.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jollyday-0.4.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/LIBRARY-LICENSES  \n",
            "   creating: stanford-corenlp-full-2018-10-05/sutime/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/british.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/defs.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/spanish.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/english.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/english.holidays.sutime.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/ejml-0.23-src.zip  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/input.txt.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/build.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/pom.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-javadoc.jar  \n",
            "   creating: stanford-corenlp-full-2018-10-05/tokensregex/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.input.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/retokenize.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.properties  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.rules.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.json-api-1.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-api-2.4.0-b180830.0359-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-models.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/protobuf.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.activation-api-1.2.0.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/StanfordDependenciesManual.pdf  \n",
            "   creating: stanford-corenlp-full-2018-10-05/patterns/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/example.properties  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/otherpeople.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/goldplaces.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/stopwords.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/presidents.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/names.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/places.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/goldnames.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/slf4j-simple.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/input.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/joda-time.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/xom.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-impl-2.4.0-b180830.0438-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/StanfordCoreNlpDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-core-2.3.0.1.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/RESOURCE-LICENSES  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.activation-api-1.2.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/slf4j-api.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/pom-java-11.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/ejml-0.23.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.json.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/Makefile  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/corenlp.sh  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/joda-time-2.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-api-2.4.0-b180830.0359.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jollyday.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/ShiftReduceDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-impl-2.4.0-b180830.0438.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/SemgrexDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/LICENSE.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User Stories Generation Tool\n"
      ],
      "metadata": {
        "id": "-WOUWJODnSnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Function"
      ],
      "metadata": {
        "id": "Cu-L9yyqW_gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The scentence can consist of:\n",
        "# Subject (nsubj) - Verb - Object (dobj), example: [The dog chased the cat] \"Active Sentence\"\n",
        "# Object (nsubjpass) - Verb - Subject (pobj), example: [The book was written by the author] \"Passive Sentence\"\n",
        "\n",
        "def get_subject_verb_object_from_sentence(token, subject_list, verb_list, object_list):\n",
        "  # If the token is a subject, add it to the subject_list\n",
        "  if(token.dep_ == \"nsubj\"):\n",
        "    subject = token\n",
        "    subject_list.append(subject)\n",
        "\n",
        "  # If the token is a prepositional object, add it to the subject_list\n",
        "  elif(token.dep_ == \"pobj\"):\n",
        "    subject = token\n",
        "    subject_list.append(subject)\n",
        "\n",
        "  # If the token is a direct object, add it to the object_list\n",
        "  elif(token.dep_ == \"dobj\"):\n",
        "    object = token\n",
        "    object_list.append(object)\n",
        "\n",
        "  # If the token is a passive subject, add it to the object_list\n",
        "  elif(token.dep_ == \"nsubjpass\"):\n",
        "    object = token\n",
        "    object_list.append(object)\n",
        "\n",
        "  # If the token is a verb, add it to the verb_list\n",
        "  elif(token.pos_ == \"VERB\"):\n",
        "    verb = token\n",
        "    verb_list.append(verb)"
      ],
      "metadata": {
        "id": "eYVaSRTMyo1i"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_subject_verb_object_list(doc, sentence, subject_list, verb_list, object_list, subject_verb_object_list):\n",
        "  # Check if the sentence is passive\n",
        "  passive_sentence = False\n",
        "  for object in object_list:\n",
        "    if(object.dep_ == \"nsubjpass\"):\n",
        "      passive_sentence = True\n",
        "  if(passive_sentence == False):\n",
        "    for subject in subject_list:\n",
        "      if(subject.dep_ == \"pobj\"):\n",
        "        subject_list.remove(subject)\n",
        "\n",
        "\n",
        "  # Step #1: iterate over each verb in the verb list\n",
        "  for verb in verb_list:\n",
        "    subject_verb_object_dictionary = {\"subject\": \"a user\", \"verb\": verb, \"object\": \"\", \"isNegative_sentence\": False}\n",
        "\n",
        "    # Iterate over the children of the verb \n",
        "    for child in verb.children:\n",
        "      # if the child is a negation, set the isNegative flag in the dictionary to True\n",
        "      if(child.dep_ == \"neg\"):\n",
        "        subject_verb_object_dictionary[\"isNegative_sentence\"] = True\n",
        "\n",
        "    # verb_subtree_list = list(verb.subtree)\n",
        "    # verb_index = verb_subtree_list.index(verb)\n",
        "\n",
        "    # get the index of the first word in the sentence \n",
        "    first_word_in_sentence_index_in_doc = sentence[0].i\n",
        "    # get the index of the last word in the sentence   \n",
        "    last_word_in_sentence_index = sentence[-1].i \n",
        "    # get the index of the verb in the doc\n",
        "    verb_index_in_doc = verb.i\n",
        "\n",
        "\n",
        "    # The sentence is passive\n",
        "    if(passive_sentence == True):\n",
        "      #print(\"The sentence is passive\")\n",
        "      verb_ancestor = \"\"\n",
        "      # Extracting the subject of the passive sentence\n",
        "      for token in doc[verb_index_in_doc:last_word_in_sentence_index+1]:\n",
        "        if(token in subject_list and token.dep_ == \"pobj\" and ((list(token.ancestors))[0]).lemma_ == \"by\"):\n",
        "          token_ancestors_list = list(token.ancestors)\n",
        "          for ancestor in token_ancestors_list:\n",
        "            if(ancestor.pos_ == \"VERB\"):\n",
        "              verb_ancestor = ancestor\n",
        "              break\n",
        "          if(str(verb_ancestor) != \"\"): \n",
        "            if(verb_ancestor == verb):\n",
        "              subject_verb_object_dictionary[\"subject\"] = token\n",
        "              break\n",
        "            elif(verb in list(verb_ancestor.conjuncts)):\n",
        "              subject_verb_object_dictionary[\"subject\"] = token\n",
        "              break \n",
        "            else:\n",
        "              break\n",
        "\n",
        "      # Extracting the object of the passive sentence\n",
        "      temp_reversed_list = list(doc[first_word_in_sentence_index_in_doc:verb_index_in_doc+1])\n",
        "      for token in temp_reversed_list[::-1]:\n",
        "        if(token in object_list and token.dep_ == \"nsubjpass\"):\n",
        "          token_ancestors_list = list(token.ancestors)\n",
        "          for ancestor in token_ancestors_list:\n",
        "            if(ancestor.pos_ == \"VERB\"):\n",
        "              verb_ancestor = ancestor\n",
        "              break\n",
        "          if(str(verb_ancestor) != \"\"): \n",
        "            if(verb_ancestor == verb):\n",
        "              subject_verb_object_dictionary[\"object\"] = token\n",
        "              break\n",
        "            elif(verb in list(verb_ancestor.conjuncts)):\n",
        "              subject_verb_object_dictionary[\"object\"] = token\n",
        "              break \n",
        "            else:\n",
        "              break\n",
        "\n",
        "    # The sentence is active\n",
        "    elif(passive_sentence == False):\n",
        "      #print(\"The sentence is active\")\n",
        "      verb_ancestor = \"\"\n",
        "\n",
        "      # Extracting the object of the active sentence  \n",
        "      for token in doc[verb_index_in_doc:last_word_in_sentence_index+1]:\n",
        "        if(token in object_list and token.dep_ == \"dobj\"):\n",
        "          token_ancestors_list = list(token.ancestors)\n",
        "          for ancestor in token_ancestors_list:\n",
        "            if(ancestor.pos_ == \"VERB\"):\n",
        "              verb_ancestor = ancestor\n",
        "              break\n",
        "          if(str(verb_ancestor) != \"\"): \n",
        "            if(verb_ancestor == verb):\n",
        "              subject_verb_object_dictionary[\"object\"] = token\n",
        "              break\n",
        "            elif(verb in list(verb_ancestor.conjuncts)):\n",
        "              subject_verb_object_dictionary[\"object\"] = token\n",
        "              break           \n",
        "            else:\n",
        "              break\n",
        "\n",
        "      # Extracting the subject of the active sentence \n",
        "      be_ancestor = False   \n",
        "      temp_reversed_list = list(doc[first_word_in_sentence_index_in_doc:verb_index_in_doc+1])\n",
        "      for token in temp_reversed_list[::-1]:\n",
        "        if(token in subject_list and token.dep_ == \"nsubj\"):\n",
        "          token_ancestors_list = list(token.ancestors)\n",
        "          for ancestor in token_ancestors_list:\n",
        "            if(ancestor.pos_ == \"VERB\"):\n",
        "              verb_ancestor = ancestor\n",
        "              break\n",
        "            elif(ancestor.lemma_ == \"be\"):\n",
        "              be_ancestor = True\n",
        "              break  \n",
        "          if(str(verb_ancestor) != \"\"):            \n",
        "            if(verb_ancestor == verb):\n",
        "              subject_verb_object_dictionary[\"subject\"] = token\n",
        "              break\n",
        "            elif(be_ancestor == True):\n",
        "              subject_verb_object_dictionary[\"subject\"] = token\n",
        "              break    \n",
        "            elif(verb in list(verb_ancestor.conjuncts)):\n",
        "              subject_verb_object_dictionary[\"subject\"] = token\n",
        "              break        \n",
        "            else:\n",
        "              break\n",
        "\n",
        "    # If the sentence has an object then it is added to the subject_verb_object_list \n",
        "    if(str(subject_verb_object_dictionary[\"object\"]) != \"\"):\n",
        "      subject_verb_object_list.append(subject_verb_object_dictionary)"
      ],
      "metadata": {
        "id": "6ADu38MK1gdJ"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_object_pronouns_in_title(subject, object):\n",
        "  # substitute \"his\" with the appropriate pronoun\n",
        "  object = re.sub(r'\\bhis\\b', subject, object)\n",
        "  # substitute \"her\" with the appropriate pronoun\n",
        "  object = re.sub(r'\\bher\\b', subject, object)\n",
        "  # substitute \"their\" with the appropriate pronoun\n",
        "  object = re.sub(r'\\btheir\\b', subject, object)\n",
        "  # remove extra spaces and parentheses\n",
        "  object = object.replace(\" ,\", \",\").replace(\"( \", \"(\").replace(\" )\", \")\")\n",
        "  # return the modified object\n",
        "  return object"
      ],
      "metadata": {
        "id": "23ZdOfRqlcTP"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_unclosed_brackets(sentence):\n",
        "  open_bracket_index = sentence.find('(')\n",
        "  # check if the unclosed bracket is found\n",
        "  if open_bracket_index != -1:\n",
        "    # search for the closing bracket\n",
        "    close_bracket_index = sentence.find(')', open_bracket_index)  \n",
        "    # check if a closing bracket is found\n",
        "    if close_bracket_index != -1:\n",
        "      # if a closing bracket is found, keep the original sentence\n",
        "      corrected_sentence = sentence  \n",
        "    else:\n",
        "      # if no closing bracket is found, remove the unclosed bracket\n",
        "      corrected_sentence = sentence[:open_bracket_index] + sentence[open_bracket_index+1:]  \n",
        "  else:\n",
        "    # if no unclosed bracket is found, keep the original sentence\n",
        "    corrected_sentence = sentence\n",
        "\n",
        "  return corrected_sentence  "
      ],
      "metadata": {
        "id": "i_TV1AgNb7Y6"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_unclosed_brackets(sentence):\n",
        "  open_bracket_index = sentence.find('(')\n",
        "  # check if the unclosed bracket is found\n",
        "  if open_bracket_index != -1:\n",
        "    # search for the closing bracket\n",
        "    close_bracket_index = sentence.find(')', open_bracket_index)  \n",
        "    # check if a closing bracket is found\n",
        "    if close_bracket_index != -1:\n",
        "      # if a closing bracket is found, keep the original sentence\n",
        "      corrected_sentence = sentence  \n",
        "    else:\n",
        "      # if no closing bracket is found, add the missing bracket\n",
        "      corrected_sentence = sentence + \")\" \n",
        "  else:\n",
        "    # if no unclosed bracket is found, keep the original sentence\n",
        "    corrected_sentence = sentence\n",
        "\n",
        "  return corrected_sentence  "
      ],
      "metadata": {
        "id": "9cwedKFnX3zn"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_if_subject_incorrect(doc, sentence, subject, verb, subject_list):\n",
        "  # get the index of the first word in the sentence\n",
        "  first_word_in_sentence_index = sentence[0].i\n",
        "  # get the index of the verb\n",
        "  verb_index = verb.i\n",
        "  # check if the subject is \"a user\"\n",
        "  if(subject == \"a user\"):\n",
        "    # look at each token from the first word in the sentence up to and including the verb\n",
        "    for token in doc[first_word_in_sentence_index:verb_index + 1]:\n",
        "      # check if the token is in the subject list, is a verb, and the next word is \"to\" and the word after that is the verb we're looking for\n",
        "      if((token in subject_list) and (token.nbor().pos_ == \"VERB\") and (token.nbor().nbor().lemma_ == \"to\") and (token.nbor().nbor().nbor() == verb)):\n",
        "        # if the conditions are met, set the subject to the text of the token\n",
        "        subject = token.text\n",
        "        # return the corrected subject\n",
        "        return subject\n",
        "  # if the subject doesn't need correction, return the original subject\n",
        "  return subject"
      ],
      "metadata": {
        "id": "Cx6RQ335av8E"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_subject_and_pronouns(subject, object):\n",
        "  # set default pronoun to \"I\"\n",
        "  pronoun = \"I\"\n",
        "\n",
        "  subject = subject.lower()\n",
        "  \n",
        "  # check if subject is a specific pronoun and set subject and pronoun accordingly\n",
        "  if(subject == \"i\" or subject == \"he\" or subject == \"she\" or subject == \"you\"):\n",
        "    subject = \"a user\"\n",
        "    pronoun = \"I\"\n",
        "  elif(subject == \"we\" or subject == \"they\" or subject == \"users\"):\n",
        "    subject = \"users\"\n",
        "    pronoun = \"We\"\n",
        "\n",
        "  # replace possessive pronouns with the correct form\n",
        "  object = re.sub(r'\\bhis\\b', 'my', object)\n",
        "  object = re.sub(r'\\bher\\b', 'my', object)\n",
        "  object = re.sub(r'\\btheir\\b', 'our', object)\n",
        "  \n",
        "  # remove extra spaces and parentheses\n",
        "  object = object.replace(\" ,\", \",\").replace(\"( \", \"(\").replace(\" )\", \")\")\n",
        "  \n",
        "  # return updated subject, pronoun, and object\n",
        "  return subject, pronoun, object"
      ],
      "metadata": {
        "id": "RRYgsfiJlROY"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deal_with_both_in_subject_if_found(doc, sentence, subject):\n",
        "  # check if \"both\" is found in the subject\n",
        "  both_found_in_subject = False\n",
        "  subject_list = subject.split()\n",
        "  for word in subject_list:\n",
        "    if(word.lower() == \"both\"):\n",
        "      both_found_in_subject = True\n",
        "  if(both_found_in_subject == True):\n",
        "    # get the index of the first token in the sentence\n",
        "    first_token_in_sentence_index = sentence[0].i\n",
        "    # get the index of the token with lemma \"both\" in the doc\n",
        "    for token in sentence:\n",
        "      if(token.lemma_ == \"both\"):\n",
        "        both_index_in_doc = token.i\n",
        "    # find the conjuncts in the subject and replace \"both\" with the conjuncts\n",
        "    for token in doc[first_token_in_sentence_index:both_index_in_doc]:\n",
        "      if(token.dep_ == \"conj\"):\n",
        "        # get the raw text of the conjuncts\n",
        "        conjuncts_raw_tokens = []\n",
        "        for item in token.conjuncts[0].subtree:\n",
        "          conjuncts_raw_tokens.append(item.text)\n",
        "        conjuncts_raw_text = \" \".join(conjuncts_raw_tokens)\n",
        "        # get the index of \"both\" in the subject\n",
        "        both_index_in_subject = subject_list.index('both')\n",
        "        # replace \"both\" with the conjuncts in the subject\n",
        "        subject = f\"{' '.join(subject_list[:both_index_in_subject + 1])} ({conjuncts_raw_text.lower()}) {' '.join(subject_list[both_index_in_subject + 1:])}\"\n",
        "  return subject\n"
      ],
      "metadata": {
        "id": "Raer_ZAIb24p"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deal_with_the_sentence_after_object(doc, sentence, subject, verb, object, original_object, subject_verb_object_list):\n",
        "  # print(\"object: {}\".format(object)) \n",
        "  # get the subtree of the original object\n",
        "  original_object_subtree = list(original_object.subtree)\n",
        "  # get the last item in the original object's subtree    \n",
        "  last_item_in_object_subtree = original_object_subtree[-1]  \n",
        "  # get the index of the first word in the sentence \n",
        "  first_word_in_sentence_index = sentence[0].i \n",
        "  # get the index of the last word in the sentence   \n",
        "  last_word_in_sentence_index = sentence[-1].i    \n",
        "  # iterate through the tokens in the sentence\n",
        "  for token in doc[first_word_in_sentence_index:last_word_in_sentence_index+1]:  \n",
        "    # if the current token matches the last item in the original object's subtree \n",
        "    if(token.text == last_item_in_object_subtree.text):\n",
        "      if(token in list(verb.subtree)):    \n",
        "        # set the index of the last word in the object to the index of the current token\n",
        "        last_word_in_object_index = token.i\n",
        "        break   \n",
        "\n",
        "  # print(doc[last_word_in_object_index:last_word_in_sentence_index+1]) \n",
        "  for token in doc[last_word_in_object_index:last_word_in_sentence_index+1]:\n",
        "    # check if the token is an ADP (preposition)\n",
        "    if(token.pos_ == \"ADP\" and token.lemma_ != \"to\" and token.lemma_ != \"so\" and token.lemma_ != \"that\"):\n",
        "      # get the subtree of the verb\n",
        "      verb_subtree_of_object = list(verb.subtree)\n",
        "      if(original_object in verb_subtree_of_object):\n",
        "        # get the index of the original object in the verb subtree\n",
        "        object_in_verb_subtree_index = verb_subtree_of_object.index(original_object)\n",
        "        verb_subtree_of_object_list = verb_subtree_of_object[object_in_verb_subtree_index:]\n",
        "      else:\n",
        "        verb_subtree_of_object_list = verb_subtree_of_object\n",
        "      \n",
        "      # loop through the tokens in the verb subtree that come after the original object\n",
        "      for adp_item in verb_subtree_of_object_list:\n",
        "        # check if the token is an preposition\n",
        "        if(adp_item.pos_ == \"ADP\"):\n",
        "          # create a list of the raw tokens in the preposition subtree\n",
        "          adp_item_raw_tokens = []\n",
        "          for item in adp_item.subtree:\n",
        "            adp_item_raw_tokens.append(item.text)\n",
        "          # join the raw tokens to create the raw text of the preposition\n",
        "          adp_item_raw_text = \" \".join(adp_item_raw_tokens)\n",
        "          # remove extra spaces and parentheses\n",
        "          adp_item_raw_text = adp_item_raw_text.replace(\" ,\", \",\").replace(\"( \", \"(\").replace(\" )\", \")\")\n",
        "          # loop through the ancestors of the preposition token to find the verb\n",
        "          for item in list(adp_item.ancestors):\n",
        "            if(item.pos_ == \"VERB\"):\n",
        "              adp_verb = item\n",
        "              break\n",
        "          # check if the verb of the preposition is the same as the input verb\n",
        "          if(verb == adp_verb):\n",
        "            # check if the raw text of the preposition is not already in the object\n",
        "            if(adp_item_raw_text.lower() not in object.lower()):\n",
        "              # check if the subject is not in the raw text of the preposition\n",
        "              if(subject.lower() not in adp_item_raw_text.lower()):\n",
        "                # add the raw text of the preposition to the object\n",
        "                # print(\"adp_item_raw_text: {}\".format(adp_item_raw_text))\n",
        "                object = f\"{object} {adp_item_raw_text}\"\n",
        "                return object\n",
        "            else:\n",
        "              return object\n",
        "\n",
        "\n",
        "    # Check if the token is the \"to\" lemma\n",
        "    elif(token.lemma_ == \"to\"): \n",
        "      # print(\"token: {}\".format(token)) \n",
        "      # Initialize a list to store the tokens of the \"to\" subtree\n",
        "      to_sentence_raw_tokens = [] \n",
        "      # Loop over the subtree of the head of the \"to\" token\n",
        "      for item in token.head.subtree: \n",
        "        # Append the text of each token to the list\n",
        "        to_sentence_raw_tokens.append(item.text)\n",
        "        # Join the list of tokens into a string\n",
        "        to_sentence_raw_text = \" \".join(to_sentence_raw_tokens)\n",
        "      # Split the string into a list of words \n",
        "      to_sentence_raw_text_list = to_sentence_raw_text.split() \n",
        "      # Find the index of the \"to\" word in the list\n",
        "      to_index = to_sentence_raw_text_list.index(\"to\") \n",
        "      # Append the words after \"to\" to the object\n",
        "      object = f\"{object} {' '.join(to_sentence_raw_text_list[to_index:])}\" \n",
        "      # Return the modified object string\n",
        "      return object \n",
        "\n",
        "    # Check if the token is the conjunction \"so\" and it is not followed by \"that\"\n",
        "    elif(token.lemma_ == \"so\" and token.nbor().text != \"that\"):  \n",
        "      so_sentence_raw_tokens = []\n",
        "      # Iterate over the subtree of the head of the \"so\" token\n",
        "      for item in token.head.subtree:  \n",
        "        so_sentence_raw_tokens.append(item.text)\n",
        "        so_sentence_raw_text = \" \".join(so_sentence_raw_tokens)\n",
        "      # Convert the raw text to a list of words\n",
        "      so_sentence_raw_text_list = so_sentence_raw_text.split()\n",
        "      # Get the index of the \"so\" conjunction  \n",
        "      so_index = so_sentence_raw_text_list.index(\"so\")  \n",
        "      # Append the words after the \"so\" conjunction to the object phrase\n",
        "      object = f\"{object} {' '.join(so_sentence_raw_text_list[so_index:])}\"  \n",
        "      # Return the modified object string      \n",
        "      return object  \n",
        "\n",
        "\n",
        "    elif(token.lemma_ == \"that\"):\n",
        "      # create an empty list to store the raw tokens of the sentence after 'that'\n",
        "      that_sentence_raw_tokens = []\n",
        "      # iterate over the subtree of the head of the token\n",
        "      for item in token.head.subtree:\n",
        "        # append the text of each item in the subtree to the list of raw tokens\n",
        "        that_sentence_raw_tokens.append(item.text)\n",
        "        # join the list of raw tokens with spaces to create a string of the entire sentence\n",
        "        that_sentence_raw_text = \" \".join(that_sentence_raw_tokens)\n",
        "      # split the string of the entire sentence into a list of individual words\n",
        "      that_sentence_raw_text_list = that_sentence_raw_text.split()\n",
        "      # find the index of 'that' in the list of individual words\n",
        "      that_index = that_sentence_raw_text_list.index(\"that\")\n",
        "      # check if the word before 'that' is 'so'\n",
        "      if(that_sentence_raw_text_list[that_index - 1].lower() == \"so\"):\n",
        "        # if it is, add 'so' and the words after 'so' to the object\n",
        "        object = f\"{object} {' '.join(that_sentence_raw_text_list[that_index - 1:])}\"\n",
        "      else:\n",
        "        # if it's not, add 'that' and the words after 'that' to the object\n",
        "        object = f\"{object} {' '.join(that_sentence_raw_text_list[that_index:])}\"\n",
        "      # return the updated object\n",
        "      return object\n",
        "\n",
        "      \n",
        "    # If the current token is an opening bracket and its head word is in the object\n",
        "    elif(token.lemma_ == \"(\"):\n",
        "      if(str(token.head) in object):\n",
        "        # Get the index of the opening bracket\n",
        "        open_bracket_index = token.i\n",
        "        # Find the closing bracket in the rest of the sentence\n",
        "        for token in doc[open_bracket_index:last_word_in_sentence_index+1]:\n",
        "          if(token.lemma_ == \")\"):\n",
        "            closed_bracket_index = token.i\n",
        "            break\n",
        "        # Get the span of tokens between the brackets\n",
        "        brackets_span = doc[open_bracket_index:closed_bracket_index+1]\n",
        "        # Convert the span into a list of tokens and then into a sentence\n",
        "        brackets_span_list = [t.text for t in brackets_span]\n",
        "        brackets_span_sentence = ' '.join(brackets_span_list)\n",
        "        # Get the span of tokens inside the brackets\n",
        "        inside_brackets_span = doc[open_bracket_index+1:closed_bracket_index]\n",
        "        # Convert the inside span into a list of tokens and then into a sentence\n",
        "        inside_brackets_span_list = [t.text for t in inside_brackets_span]\n",
        "        inside_brackets_span_sentence = ' '.join(inside_brackets_span_list)\n",
        "        inside_brackets_span_sentence = inside_brackets_span_sentence.replace(\" ,\", \",\").replace(\"( \", \"(\").replace(\" )\", \")\")\n",
        "        # If the inside span sentence is not already in the object, add the brackets span sentence\n",
        "        if(inside_brackets_span_sentence not in object):\n",
        "          object = f\"{object} {brackets_span_sentence}\"\n",
        "          # Clean up the object string by replacing multiple spaces and parentheses with single spaces and parentheses\n",
        "          object = object.replace(\"  \", \" \").replace(\"( \", \"(\").replace(\" )\", \")\")\n",
        "          return object\n",
        "      # Exit the loop once the first opening bracket whose head is in the object is found\n",
        "      break\n",
        "\n",
        "  # If none of condtion above is satsified, return the original object string\n",
        "  return object"
      ],
      "metadata": {
        "id": "GjPXut_La1iN"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def add_missing_words_after_object(verb, object):\n",
        "#   verb_subtree_list = [t.text for t in verb.subtree]\n",
        "#   object_list = object.split()\n",
        "#   print(verb_subtree_list)\n",
        "#   print(object_list)\n",
        "\n",
        "#   last_word_in_object = object_list[-1]\n",
        "#   print(last_word_in_object)\n",
        "\n",
        "\n",
        "#   last_word_in_object_index_in_verb_subtree_list = verb_subtree_list.index(last_word_in_object)\n",
        "#   print(last_word_in_object_index_in_verb_subtree_list)\n",
        "\n",
        "#   item_index = verb_subtree_list.index(verb_subtree_list[-1])\n",
        "#   print(item_index)\n",
        "#   for item in verb_subtree_list:\n",
        "#     if(item == \".\" or item == \",\"):\n",
        "#       item_index = verb_subtree_list.index(item)\n",
        "#       print(item_index)\n",
        "\n",
        "#   missing_words_list = verb_subtree_list[last_word_in_object_index_in_verb_subtree_list + 1 : item_index + 1]\n",
        "\n",
        "\n",
        "#   print(verb_subtree_list[last_word_in_object_index_in_verb_subtree_list + 1 : item_index + 1])\n",
        "#   missing_words_text = \" \".join(verb_subtree_list[last_word_in_object_index_in_verb_subtree_list + 1 : item_index + 1])\n",
        "#   object = f\"{object} {missing_words_text}\"\n",
        "#   return object"
      ],
      "metadata": {
        "id": "xa67dwIxAUAH"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_subject_verb_object_list(doc, sentence, subject_list, verb_list, object_list, subject_verb_object_list, user_stories_components_list):\n",
        "\n",
        "  for subject_verb_object_dict in subject_verb_object_list:\n",
        "\n",
        "    subject, pronoun, verb, verb_in_title, object, object_in_title = \"\", \"\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "    subject = subject_verb_object_dict[\"subject\"]\n",
        "\n",
        "    if(str(subject) != \"a user\"):\n",
        "      # If the subject consists of more than one token, join all the token to form one subject\n",
        "      subject_raw_tokens = []\n",
        "      for item in subject.subtree:\n",
        "        subject_raw_tokens.append(item.text)\n",
        "        subject_raw_text = \" \".join(subject_raw_tokens)\n",
        "\n",
        "      subject = subject_raw_text.replace(\" ,\", \",\").replace(\"( \", \"(\").replace(\" )\", \")\")\n",
        "\n",
        "    object = subject_verb_object_dict[\"object\"]\n",
        "\n",
        "    # If the object consists of more than one token, join all the token to form one object\n",
        "    object_raw_tokens = []\n",
        "    for item in object.subtree:\n",
        "      object_raw_tokens.append(item.text)\n",
        "      object_raw_text = \" \".join(object_raw_tokens)\n",
        "\n",
        "    object = object_raw_text.replace(\" ,\", \",\").replace(\"( \", \"(\").replace(\" )\", \")\")\n",
        "\n",
        "    object_in_title = set_object_pronouns_in_title(subject, object)\n",
        "    object_in_title = remove_unclosed_brackets(object_in_title)\n",
        "    object_in_title = object_in_title.rstrip(',')\n",
        "\n",
        "\n",
        "    verb_in_title = subject_verb_object_dict['verb'].lemma_\n",
        "\n",
        "\n",
        "    subject = check_if_subject_incorrect(doc, sentence, subject, subject_verb_object_dict['verb'], subject_list)\n",
        "    subject, pronoun, object = set_subject_and_pronouns(subject, object)\n",
        "    subject = deal_with_both_in_subject_if_found(doc, sentence, subject)\n",
        "\n",
        "\n",
        "    object = remove_unclosed_brackets(object)\n",
        "    object = deal_with_the_sentence_after_object(doc, sentence, subject, subject_verb_object_dict['verb'], object, subject_verb_object_dict['object'], subject_verb_object_list)\n",
        "    object = fix_unclosed_brackets(object)\n",
        "    # object = add_missing_words_after_object(subject_verb_object_dict['verb'], object)\n",
        "    object = object.rstrip(' .').replace(\"  \", \" \").rstrip(' ,')\n",
        "\n",
        "\n",
        "    verb = subject_verb_object_dict['verb'].lemma_\n",
        "\n",
        "\n",
        "    isNegative_sentence = subject_verb_object_dict[\"isNegative_sentence\"]\n",
        "\n",
        "    user_stories_components_dictionary = {\"subject\": subject, \"pronoun\": pronoun, \"verb\": verb, \"verb_in_title\": verb_in_title, \"object\": object, \"object_in_title\": object_in_title, \"isNegative\": isNegative_sentence}\n",
        "\n",
        "    user_stories_components_list.append(user_stories_components_dictionary)"
      ],
      "metadata": {
        "id": "UTYk9JSFOe0Y"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes a parsed document and a sentence within it as input\n",
        "def check_if_negative_sentence(doc, sentence):\n",
        "  # Set the variable 'negative_sentence' to False initially\n",
        "  negative_sentence = False\n",
        "  # Initialize an empty list 'verbs_finder'\n",
        "  verbs_finder = []\n",
        "  # Iterate over each token in the input sentence\n",
        "  for token in sentence:\n",
        "    # Check if the token is a negation modifier\n",
        "    if(token.dep_ == \"neg\"):\n",
        "      # If it is a negation modifier, iterate over each word in the sentence up to the negation modifier\n",
        "      for word in doc[sentence[0].i:token.i]:\n",
        "        # If the word is a verb, append it to 'verbs_finder'\n",
        "        if(word.pos_ == \"VERB\"):\n",
        "          verbs_finder.append(word)\n",
        "      # If there are no verbs in 'verbs_finder', it means that the negation modifier is negating the entire sentence\n",
        "      if(len(verbs_finder) == 0):\n",
        "        # In this case, set 'negative_sentence' to True\n",
        "        negative_sentence = True\n",
        "  # Return the value of 'negative_sentence'\n",
        "  return negative_sentence"
      ],
      "metadata": {
        "id": "-SOY8BdEl_JV"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_user_stories_from_sentence(doc, sentence, subject_verb_object_list, user_stories_components_list, user_stories_list):\n",
        "\n",
        "  print(\"subject_verb_object_list: {}\".format(subject_verb_object_list))\n",
        "\n",
        "  print(\"user_stories_components_list: {}\".format(user_stories_components_list))\n",
        "\n",
        "  negative_sentence = check_if_negative_sentence(doc, sentence)\n",
        "\n",
        "  for user_stories_components_dict in user_stories_components_list:\n",
        "    # generate a unique id using uuid\n",
        "    id = uuid.uuid4()\n",
        "    user_stories_dictionary = {\"userStoryTitle\": \"\", \"userStoryDescription\": \"\", \"userStoryID\": id}\n",
        "\n",
        "    subject = user_stories_components_dict[\"subject\"]\n",
        "    pronoun = user_stories_components_dict[\"pronoun\"]\n",
        "    verb = user_stories_components_dict[\"verb\"]\n",
        "    verb_in_title = user_stories_components_dict[\"verb_in_title\"]\n",
        "    object = user_stories_components_dict[\"object\"]\n",
        "    object_in_title = user_stories_components_dict[\"object_in_title\"]\n",
        "\n",
        "    user_stories_dictionary[\"userStoryTitle\"] = f\"{verb_in_title.title()} {object_in_title.title()}\"\n",
        "\n",
        "    \n",
        "    if(user_stories_components_dict[\"isNegative\"] == False and negative_sentence == False):\n",
        "      user_stories_dictionary[\"userStoryDescription\"] = f\"As {subject}, {pronoun} want to be able to {verb} {object}.\"\n",
        "\n",
        "    elif(user_stories_components_dict[\"isNegative\"] == False and negative_sentence == True):\n",
        "      user_stories_dictionary[\"userStoryDescription\"] = f\"As {subject}, {pronoun} should not be able to {verb} {object}.\"\n",
        "\n",
        "    elif(user_stories_components_dict[\"isNegative\"] == True):\n",
        "      user_stories_dictionary[\"userStoryDescription\"] = f\"As {subject}, {pronoun} should not be able to {verb} {object}.\"\n",
        "\n",
        "    user_stories_list.append(user_stories_dictionary)\n",
        "    \n",
        "  print(\"user_stories_list: {}\".format(user_stories_list))"
      ],
      "metadata": {
        "id": "p6BJczahzOl7"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case Studies "
      ],
      "metadata": {
        "id": "fO1W0Jr7CqhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = \"Finding and classifying repetitive DNA sequence in eukaryotic genomes is both an important first step ahead of further genome annotation, and also interesting in its own right as repeats frequently drive genome evolution. Repeats in DNA can be broken into a number of different major classes such as LINEs, SINEs and LTRs. Global biodiversity efforts such as Darwin Tree of Life, the European Reference Genome Atlas and the Earth BioGenome Project are producing hundreds and soon thousands of high-quality reference genomes, that will all need repeat annotation. Currently we have two potential approach to annotating repeats. The first is building a repeat library for a species (using RepeatModeler) and then annotating the repeats on the genome (using RepeatMasker). This method both finds and classifies the repeats and finds lineage specific repeats, however building a repeat library is computationally costly. The second approach is to use an extremely fast k-mer approach (REpeatDetector, aka Red), to mask the genome in a fraction of this time. The downside is that this approach does not classify repeats and so is not very informative for researchers studying repeat evolution. In this project we want to explore Deep Learning in order to help classify repeats. We have large existing training sets across hundreds of species, spanning billions of classified repeats. As part of this projects you would train a neural network to take as input an unclassified repeat sequence and label it according to the class of repeats it belongs. You will explore the most efficient approach in terms of both preparing the training data and constructing the network. If the training is successful, we will then test the resulting model from a perspective of compute efficiency, i.e. does the model produce similar results to our existing method of classification (i.e. building a repeat library for the species and then using it to find and classify repeats) and what is the relative compute cost in each approach. Depending on the success and progress related to the above, there may also be the opportunity to take the project a step further, in terms of generative repeat library construction, i.e. given a fast k-mer derived set of repeat sequences and their coordinates on the genome, is it possible to generate a repeat library. This would be highly experimental and only considered after fast and excellent progress on the core project.\"\n",
        "\n",
        "text_2 = \"Finding and classifying repetitive DNA sequence in eukaryotic genomes is both an important first step ahead of further genome annotation, and also interesting in its own right as repeats frequently drive genome evolution. Global biodiversity efforts such as Darwin Tree of Life, the European Reference Genome Atlas and the Earth BioGenome Project are producing hundreds and soon thousands of high-quality reference genomes, that will all need repeat annotation. Currently our repeat annotation pipelines are run via an in-house workflow management system, eHive. eHive is Perl based and nearing end of life and as a result we are transitioning much or our infrastructure to other workflow managers such as Nextflow. In this project you will work together with us to help redesign our repeat annotation pipeline. We will identify all the existing components, decide what to keep and what to remove and then come up with a final workflow. You will then implement this workflow using Nextflow and test the deployment both locally and on our various cloud partners. Time permitting we will work on costing the pipeline using a variety of species to come up with a cost per gigabase of sequence to mask repeats. Similarly, if there is additional time, we will look at large scale deployment of the pipeline on our species to build a consistent set of repeat resources for public use.\"\n",
        "\n",
        "text_3 = \"Protein-coding genes form the basis of many scientific analyses. They have directly links to important real world problems such as human health, food security and ecosystem conservation. Global biodiversity efforts such as Darwin Tree of Life, the European Reference Genome Atlas and the Earth BioGenome Project are producing hundreds and soon thousands of high-quality reference genomes, and these genomes need structural annotation of genes.\"\n",
        "\n",
        "text_4 = \"Testing is a crucial part of the software development process, as it helps to ensure the quality and functionality of the software. It helps to identify bugs, improve the user experience, and it ensures that the software meets the specified requirements and standards. Query languages have well-defined syntax and require specific behavior of the database system. Instead of adding an entire new feature, this project aims at improving the test coverage for the cypher query language in Polypheny-DB. By following the official documentation of the openCypher query language and by systematically adding test cases, existing bugs can be identified, and future regressions can be avoided\"\n",
        "\n",
        "text_5 = \"For some applications, especially for those making use of the multimedia and file storage capabilities of Polypheny-DB, it is useful to represent and interact with a table (or the result of an arbitrary query) as file system. With Query to File we already have a prototype implementation of this using FUSE and running on the client computer. The idea of this project is to integrate this concept directly into Polypheny-DB. Instead of an application running on the local machine,\"\n",
        "\n",
        "text_6 = \"Currently, there is a JDBC driver and a Python connector for Polypheny-DB. In this project, support for other languages or frameworks shall be added. This project is explicitly for developers with experience with interacting with databases in a specific language or framework. Feel free to link references to experience with that language or framework in your proposal.\"\n",
        "\n",
        "text_7 = \"Polypheny-DB visualizes query plans in its user interface. Although this feature is very powerful and provides various insights, there is potential for visual improvements. This project idea is about visually improving the plan view. This might include adding the estimated number of row to the edges or making the thickness of the edges depending on it. A proposal for this project idea should include a concept on the planned changes.\"\n",
        "\n",
        "text_8 = \"CouchDB is a popular document-oriented database system. It features an HTTP query interface that allows querying and manipulating data. The idea of this project is to build a query interface for Polypheny-DB that adheres to the specification of the CouchDB query API. This would allow to seamlessly replace an CouchDB database with Polypheny-DB or to use applications written for ChouchDB with Polypheny-DB.\"\n",
        "\n",
        "text_9 = \"SPARQL is a query language for RDF graphs and is one of the key technologies of the semantic web. It is used to perform operations such as selecting, inserting, updating, and deleting data. SPARQL is similar to SQL, but it is specifically designed for querying RDF data. The idea of this project is to add native support for SPARQL to Polypheny-DB. Since Polypheny-DB uses the Label-Property-Graph (LPG) model to represent graphs, implementing some kind of mapping will be necessary.\"\n",
        "\n",
        "text_10 = \"Data source adapters allow to map existing data into the schema of Polypheny-DB. This allows to query the mapped data using the available query languages and features of Polypheny-DB. Furthermore, imported entities can be combined (e.g., joined or unioned) with other tables. The goal of this project is to add an adapter similar to the CSV adapter but for XML files. You can also come up with your own idea for a data source adapter. Data source adapters do not necessarily \"\n",
        "\n",
        "text_11 = \"This project is similar to the data source adapter for XML Files. However, instead of querying a static file, the idea of this project is to build an adapter that accesses WikiData. WikiData is a free, open, multilingual knowledge graph maintained by the Wikimedia Foundation. It is a structured database of data that can be linked and reused across multiple Wikipedia projects and other websites. The data in WikiData includes information about entities such as people, places, organizations, and works of art, as well as information about relationships between entities and facts about those entities. The whole database can be downloaded in different formats. This project has the potential to be extended into a large project.\"\n",
        "\n",
        "text_12 = \"LDAP (Lightweight Directory Access Protocol) is a widely used, open and vendor-neutral, industry standard application protocol for accessing and maintaining distributed directory information services over an Internet Protocol (IP) network. It provides a common interface for accessing and manipulating directory information, such as usernames and passwords, email addresses, and other directory-based information. By adding support for querying Polypheny-DB using LDAP, Polypheny-DB can seamlessly be integrated in applications using LDAP.\"\n",
        "\n",
        "text_13 = \"Generating the expected HTTP responses is a difficult task. The student is expected to study Jenkins core to identify ways to extract them. For example, they could be extracted from Javadocs and annotations. As part of the community bonding and student project proposal phase, the student is expected to make a few proposals on how to specify and generate the REST API for the Jenkins core and for the plugins. In the case the student finds that it is not possible to generate the REST API from a specification, the student should identify why this is not possible. We also ask the student to explore and propose a way to have REST API of plugins be generated from a REST API specification. For example, some auto-code could populate what the javadoc would look like in an empty-plugin used by the maven plugin generator. The student is also expected to study and propose how the REST API documentation generation could be part of the REST API generator. It might be helpful to automatically generate some code for the REST API when the plugin developer creates a plugin for the first time using the plugin skeleton generator. Any methodology created to handle the REST API should be built into the skeleton generator. The jenkins core REST API and the plugins own REST API need to be versioned separately. It is suggested to focus first on generating the specification, then later look at the versioning of the REST API. Nested objects make versioning challenging. Jenkins users should be easily able to see the REST APIs available for their installed Jenkins. For Jenkins core, this could be done with a URL like: http://localhost/rest/api/1.0. The plugins would have their own REST API path with a version number like: http://localhost/plugin/rest/api/1.0. Plugins and the core would thus have their own version number, and an additional REST API version number. Automated API documentation using the OpenAPI 3.0 specification is part of identifying the API specs.\"\n",
        "\n",
        "text_14 = \" Moira’s users are able to set up new delivery channels and contacts to be used with those channels. However Moira doesn’t check if the channel configuration is valid and alerts can be actually sent. A user may provide a non-existent Slack user name, block Moira’s bot in Telegram, etc. As a result, such user wouldn’t be able to receive alerts. The bad thing is that sometimes invalid configuration would cause Moira’s bots to be banned for a certain period of time. This effectively means a denial-of-service for alerts which is highly undesirable. The aim of this project is to implement health checks when delivery channels and contacts are set up. To do so, one should enhance the delivery channel and contact setup flow: send a test alert, verify that it’s received, don’t let to save an invalid configuration otherwise. Certain modifications of the web UI may be required.\"\n",
        "\n",
        "text_15 = \"Moira designed to be API-first solution and all the setup of alerting must be done via HTTP API. Unfortunately Moira’a API right now is not follow all the principles of REST. This means that HTTP methods somewhere are not used correctly and URL paths somewhere are not describe the resources in a right way. Additionally some of the endpoints provide the data which schema is overcomplicated and contains wrong attributes. The great solution for this type of issues will be to use JSON API standard. The aim of this project is to define methods of API that do not follow to the and change it using the REST and JSON API principles.\"\n",
        "\n",
        "text_16 = \"Explanation. Moira is a huge and complicated software and it operates with a huge amount of data. Sometimes for statistics and troubleshooting we need to define some metrics that will more precise tell us which amount of load Moira is carrying on. The example of this metrics is: amount of triggers with tagged metrics, amount of triggers with huge amount of metrics, amount of triggers with and without subscriptions, etc.To achieve this goal we can create a new microservice that will collect this data from storage and export it to graphite or implement this metrics to existing services.\"\n",
        "\n",
        "text_17 = \"To provide best user experience Moira’s web UI were developed with accuracy and meant to be as much minimalistic and laconic as possible. But still there are exist pages that do not look perfectly in mobile version of web interface. The example of this pages are following pages: Main page and navigation on it, Subscriptions page, Trigger page and trigger edit pages, Teams page. The aim of this project is to add this pages to mobile version of Moira’s web UI and build the UI with best user experience in mobile environment.\"\n",
        "\n",
        "text_18 = \"On-call engineers are badly affected by noisy triggers that generate alerts multiple times a day. Attention to alerts reduces greatly, and chances to miss one important alert grow. One badly configured flapping trigger can affect the entire workflow. Our documentation contains an entire page dedicated to this problem with some tips on mitigation. But we can do more. The aim of this project is to help Moira users identify noisy triggers. To do so, one should research and define a metric of trigger noisiness, and then create a UI page that demonstrates worst triggers to the user.\"\n",
        "\n",
        "text_19 = \"Moira’s web UI is nice and widely used. However, users don’t always want to create triggers, subscriptions, and contacts manually. They would like to be able to automate routine tasks with the tools like Ansible which they already use to bootstrap database and application clusters. For this kind of automation, Moira should have a well-documented API and a number of client libraries for all popular languages. At this point, Moira doesn’t have any API documentation. To use the API, one should study Moira’s source code or an existing client library source code to understand how the API works and reverse-engineer contracts of its methods. The aim of this project is to provide an always up-to-date documentation of Moira’s API and a few client libraries. To do so, one should create an OpenAPI description of API, generate a number of client libraries for popular programming languages with Swagger tools, and setup a process so the documentation and the clients are updated when a new API version is released.\"\n",
        "\n",
        "text_20 = \"Genes form the basis of many scientific analyses. They have directly links to important real world problems such as human health, food security and ecosystem conservation. Global biodiversity efforts such as Darwin Tree of Life, the European Reference Genome Atlas and the Earth BioGenome Project are producing hundreds and soon thousands of high-quality reference genomes, and these genomes need structural annotation of genes. Genes are made up of many different features, but exons are arguably the key feature as they represent the blocks of the genome that are transcribed in to RNA, which may form functional structures, regulate the expression of other genes or encode proteins. Under certain conditions exons may be included or skipped in the transcribed RNA, sometimes leading to different functional outcomes. A particular permutation of exons that forms a transcribed RNA is known as a transcript. While there is often on particular transcript that represents the normal state of the gene, and thus is most prevalent, it is very common to have alternative transcripts expressed, particularly in higher eukaryotes. These may be expressed in different tissues or points in time, or simply expressed continuously but at a lower level to the dominant transcript. It is important to have as complete a representation of the full set of transcripts in a gene as possible. Short read sequencing is a common method for finding alternative transcript structures, however the nature of the technology means we cannot be certain that the permutations of exons we infer from short read data actually exist in reality. Long read data allows us to directly observer full length RNA and thus should allow us to confidently identify alternative transcripts, but the technology is less common place and also does not capture as many genes as short read data. There are also frequently fragmented data present. The objective of this project will be to examine methods of better representing potential full length transcripts via deep learning. We will preform our test in mammals, where there are several high-quality reference annotations (human and mouse in particular). We will take genes from mammals where large quantities of long read data are available and identify high confidence sets of alternative transcripts. We will then utilise the union of the exons described in these transcripts to attempt to help train a model capable of validating alternative transcripts. There are two approaches we could take, the first would be to find the longest possible exon chain, assume this is the dominant transcript and automatically generate a set of alternative transcripts with exon skipping, where the model would produce a binary output as to whether or not a permutation was valid. This approach would be sraightforward, but as some genes can have many exons, this could generate many permutations. The other approach would be to try and build a generative model, where the input is the union of all unique exons across the input set, while the output would be a set of transcripts and the exons contained in each. This would be more robust, but would require a more complex model. The project will involve you working with us to identify suitable training data from our existing annotations and assessing and implementing a suitable approach to using the data to train a model. Your work will help decide which approach is most viable and you will be responsible for implementing and training the corresponding model. We will test the resulting model in terms of how accurately it can validate true alternative transcripts in both gold standard and non-model mammalian species. Time permitting we may consider extending past mammals into other eukaryotes to see how generalisable it is\"\n",
        "\n",
        "text_21 = \"Untranslated regions (UTRs) represent the boundaries of protein-coding genes. These regions are important for understanding where one gene ends and a neighbouring gene starts. UTR regions sometimes house features that regulate the expression of the gene in addition to being key to analysing the expression of the gene when using single cell data. Annotating UTRs is difficult. It is clear from long and short read transcriptomic data that there is rarely a precise start/end to the UTRs of a gene. There are usually regions where there transcriptional machinary is more likely to attach or detach. In particular, short read data (which is most frequently available) is naturally imprecise for determining the start/end of the UTR as each read represents a small fragment of the gene. If the sampling of these small fragments is uneven, it leads to incorrect identification of the start/end. At the same time the cellular machinery for transcription is able to identify these binding/release regions despite not fundamentally changing across eukaryotes, so it should be possible to directly identify their approximate locations directly from the genome sequence. In this project we will explore the use of long read data and high-quality reference annotations to train a model to predict the location of a UTR start or end from a sequence adjacent to a coding region start/end. While it will not be possible to do this for all UTRs, particularly ones that are very long or have large introns contained within, we will be able to train to predict simple UTR start/ends within a fixed window. This will assist with better representation of UTRs, particularly in species lacking transcriptomic data. We will work together to build a training set consisting of genes where we are confident we have captured repesentative UTR boundaries. When several possible boundaries in one of these genes are present, we will select the longest UTR boundary, unless it is infrequently observed relative to the number of long reads mapped to the gene (in which case the boundary will be set to be a balance of the longest UTR observed in more than 20 percent of the reads). We will use as much of the sequence of the flanking region as possible along with the coordinate of the selected boundary, to then train the model to predict the boundary coordinates. You will be responsible for building the network and testing different hyperparameters during training. We will then compare to gold standard reference annotations and look at the approximate distance between the predicted and true boundaries to evaluate the model.\"\n",
        "\n",
        "text_22 = \"MGnify is a freely available hub for the analysis and exploration of metagenomic, metatranscriptomic, amplicon and assembly data. The resource provides rich functional and taxonomic analyses of user-submitted sequences, as well as analysis of publicly available metagenomic datasets held within the European Nucleotide Archive (ENA). The public-facing service is a React.js website backed by a Python/Django REST API, which serves metagenomics data and associated analyses via API endpoints and data files. There are also micro-services for specific tasks like sequence searches. In addition to the website, MGnify provides hosted Jupyter Notebooks to cover extra use cases and showcase how the MGnify API-provided data can be used in downstream data analysis tasks (using R and Python). Together, the website and notebooks include many data visualisation built using various technologies: Highcharts (Javascript) for website graphics like nucleotide distributions, specialised javascript components like the Integrative Genomics Viewer for genome annotations, and matplotlib and ggplot for graphics created in the Jupyter notebooks. As MGnify approach the release of our next-generation analysis pipeline, the aim is develop a reusable framework for managing these visualisations. Specifically, we aim to reuse components and libraries in as many places as possible, and to support FAIR (Findable, Accessible, Interoperable, Reusable) principles by enabling our users to easily build upon the visualisations we provide. An example could be: the MGnify website using a d3.js histogram to display protein annotation information, from where users can jump to an Observable JS Notebook with the required API fetching code and d3 visualisation code ready for them to modify to produce a graphic suitable for their own publication.\"\n",
        "\n",
        "text_23 = \"Understanding the impact of genetic variation on disease requires comprehensive gene annotation. Human genes are well characterised following more than two decades of work on their annotation, however, we know that this annotation is not complete and that new experimental methods are generating data to help us towards the goal of complete gene annotation. Long transcriptomic reads allow us to identify and annotate many new features, including the start and end of a transcript which can be combined to give information for genes. We would like to develop a pipeline to extract long transcriptomic data from the European Nucleotide Archive (ENA), map to the human reference genome and extract the terminal co-ordinates to create a growing collection of transcript start/end positions. This data will support improving the accuracy of gene annotation of individual transcripts and genes and give insight into any differences between transcript start and end sites across different tissues\"\n",
        "\n",
        "text_24 = \"Understanding the impact of genetic variation on disease requires comprehensive gene annotation. Human genes are well characterised following more than two decades of work on their annotation, however, we know that this annotation is not complete and that new experimental methods are generating data to help us towards the goal of complete gene annotation. We have developed an automated workflow to use long transcriptomic data to add novel alternatively spliced transcripts to our gene annotation. Our method uses very strict thresholds to ensure that no poor-quality models are added to the gene annotation, although as a consequence we reject significant numbers of viable novel transcripts. We want to use machine learning to recover good quality but rejected transcripts and improve the setting of initial filters for new datasets.\"\n",
        "\n",
        "text_25 = \"Ensembl Metazoa plans every release by manually collecting a list of available species from INSDC resources a few months in advance, and then going over their available information (e.g. taxonomic clade, assembly quality, annotation availability/quality, RefSeq availability, etc.) to filter out and select about 20 species that will be processed and loaded into the next Ensembl release. As an example, taxonomic information is used to highlight species that cover new clades not present in Ensembl, as well as those that bring novel information to existing clades, e.g. new locust genomes in the well-known Neoptera clade. In our plans to expand our Ensembl Metazoa resources we would like to introduce automation in the process described above to check available new species/updates from INSDC resources, as well as create a system that allows us to rank them depending on different criteria. This system should collect the data on a regular basis, e.g. monthly, and provide all the required information to easily ingest it into our production loading system, e.g. GCA, species name, strain, common name, taxonomy,… Additionally, it would be desirable if the new system could rely on our JIRA tracking system to create and update this information, so we can feed this information programmatically into our processing and loading system.\"\n",
        "\n",
        "text_26 = \"The search engine of any website can be one of the most useful tools for users to help them easily retrieve the information they are looking for. Currently, Ensembl’s search tool works based on indexed fields of our databases, that mainly covers key information, e.g. genes, species, proteins, including many synonyms for every one of them. As we plan to move to our new beta website by the end of 2023, we want to make our search engine even better so our users can enjoy the experience of using Ensembl even more. We would like to expand our Ensembl beta’s search functionality to include and support searching based on taxonomic information. In particular, we are interested in providing users a list of close relatives when a given species is requested and it is not part of Ensembl (yet), return the list of species available given a taxonomic clade instead of a species name, or find a species even when a (homotypic) synonym is provided instead of its current scientific name. The objective of this project is to create a standalone Elasticsearch tool that can handle taxonomic-related requests.\"\n",
        "\n",
        "text_27 = \"Our TA grading interface is elaborate, highly-featured, and customizable. However, the interface is visually overwhelming to new graders. Some of our TA grading features are not adequately tested by automated unit and end-to-end regression testing. Finally, the performance of these webpages is problematic for large courses due to inefficient database queries. Expected Outcomes: The goal would be to expand the automated testing of the TA Grading pages, patch bugs uncovered by this improved testing, refactor the existing code and SQL queries to improve performance, and possibly propose and execute small user interface revisions.\"\n",
        "\n",
        "text_28 = \"Currently, instructors must write a configuration as a config.json (and any necessary additional files) and upload or store these files on the local file system. We would like to provide an alternate web GUI interface for creating basic or moderately complex autograding configurations. We have preliminary support for automated creation of expected output files (from and instructor solution – currently limited to Python) and randomized test case input. This project will involve multiple modules of Submitty including web UI development, integration, documentation, additional tutorial examples, and extending output generation to instructor solutions in compiled languages. Expected Outcomes: The goal would be to streamline the assignment configuration process for non-technical instructors, relevant for use in non-computer-science/non-programming courses.\"\n",
        "\n",
        "text_29 = \"Automated testing of student submitted software carries system and security risks from malicious code but also simply buggy or inefficient code. Upper level coursework on advanced topics in computer science including networking, operating systems, and kernel development are especially complex challenges. Submitty supports a variety of tools to securely test including both sandboxing and containerization (Docker). These tools must manage and limit system resources (time, CPU, processes, memory, files, system calls, sockets, etc.) The next step is to facilitate the creation of instructor-customized container images (with specific languages, packages, databases, etc.). Care must be taken to ensure small container size and efficient performance. Advanced project idea: We would like to use Submitty to automatically test and grade homework assignments that require modifications to the operating system kernel. Before doing so on a production machine, we need to do testing to ensure the right controls are in place. Expected Outcomes: Increased usage of containerized autograding in all levels of courses. Reduced size and improved performance of containerized autograding for our autograding tutorial examples and selected real-world use cases of autograding.\"\n",
        "\n",
        "text_30 = \"Each commit and pull request to github launches continuous integration testing of a portion of the Submitty code base. We would like to expand the code coverage of our unit and integration tests. Furthermore, some of our more complex end-to-end test case are not currently run automatically with each GitHub pull request, because the system setup is too time consuming and lengthy or unpredictable running times affect test stability. We would like to optimize our use of GitHub Actions and caching so we can run all of these test cases. Expected Outcomes: Increased code coverage and stability of the Submitty CI test suite, increased automation of CI testing, increased performance (decreased running time) for CI testing through GitHub Actions.\"\n",
        "\n",
        "text_31 = \"Submitty is responsible for securing confidential information. It is important that we regularly assess the security of this data. Once a potential vulnerability is found, the system must be promptly patched and documented to prevent future problems. Expected Outcomes: Security risk assessment, identification and repair of specific security vulnerabilities, expansion and creation of continuous integration tools to prevent introduction of new vulnerabilities.\"\n",
        "\n",
        "text_32 = \"Most print jobs are sent via the print dialog of a desktop application, like evince, Chrome, LibreOffice, DarkTable, … Print dialogs are usually, like “Open …” or “Save as …” dialogs, provided by the GUI toolkits, in most cases GTK or Qt, sometimes applications come also with their own creations, like LibreOffice or Chromium. Problem here is usually not the design of the dialog itself, most are actually easy to use, but the way how they connect to CUPS (and also to other print technologies) and how well this connection code is maintained and kept up-to-date. GUI toolkit projects are large projects, often with long release cycles and all with a certain inertia, and there are things which many people are eager to work on, and others, like print dialogs, which have to be there but no one is really motivated to push their development forward and do the needed maintenance work. An important part of the maintenance of a GUI toolkit is that it interfaces well and correctly with the underlying operating system, graphics, sound, storage, …, and printing! The OS is under continuous development, things are changing all the time, components get replaced by others, printing is CUPS for 23 years, but within CUPS we have also changes, and they need to be taken care of in the print dialogs. Several years back, CUPS started to create temporary queues for driverless IPP network printers (or remote CUPS printers, which are emulations of IPP printers), which are only physically available when they are accessed (capabilities are polled or job printed). Print dialogs used an old API which did not support this, the temporary queues did not appear in the dialog, a helper daemon, cups-browsed had to convert the temporary queues into physical queues as a workaround. The correct solution had been to change the print dialogs to a newer CUPS API which supports these queues, but no one at the GUI toolkit projects has felt responsible and taken the time for this update for many years. Only recently this got fixed. This made me introducing the Common Print Dialog Backends (CPDB) back in 2017, a de-coupling of the print technology (CUPS, print-to-file, that time also Google Cloud Print) from the GUI. The GUI projects have to adopt the CPDB support only once and then OpenPrinting (or any upcoming cloud printing projects) takes care of the CPDB backend for the print technologies to be up-to-date with any changes. This way print technology projects can react quickly and are not dependent any more on the GUI toolkit’s inertia. The print dialogs of the major GUI toolkits, GTK, Qt, got CPDB support added in GSoC 2022, but several applications come with their own creation of a print dialog. AFAIK these are Firefox/Thunderbird (Mozilla), Chromium/Chrome (Google), and LibreOffice. Also these dialogs need to get CPDB support to make CPDB universal. Then we are especially prepared for the switch to CUPS 3.x which does not support PPD files any more, as the CUPS backend of CPDB is already using only CUPS APIs not handling PPD files. And we are also prepared for IPP infrastructure/cloud printing for which we also want to create a CPDB backend (see below). The contributor's task is to get CPDB into the print dialogs upstream, the UI of them does not need to be changed. Dialogs to be treated are Mozilla for Firefox and Thunderbird, Chromium/Chrome, LibreOffice, and any other application-specific dialog. For LibreOffice there was already worked on CPDB support back in 2017, but in the meantime things have changed and the dialog needs to get updated, especially for the new features of CPDB 2.x (human-readable strings/translations, option groups, …).\"\n",
        "\n",
        "text_33 = \"cups-browsed is a helper daemon for CUPS to automatically set up network printers. In the beginning it was to overcome that when CUPS from 1.6.x on used DNS-SD instead of its own browsing/broadcasting, it did not auto-setup client queues any more. With the time it got lots of more functionality: Legacy CUPS browsing/broadcasting for interoperability with CUPS 1.5.x and older (often in long-term support enterprise distros), clustering, manually and automatically, also for clusters of printers of completely different types, user has one “universal” print queue and by their option settings job goes to the correct printer. Also filtering lists of many printers is supported, and everything can be configured/fine-tuned by the user or admin. With CUPS already having its temporary queue functionality for network printers without need of explicit manual setup, and the Common Print Dialog Backends getting into the print dialogs and talking to CUPS with modern interfaces, we do not need automatic queue creation for network printers any more, but the other functionality of cups-browsed is still very useful. So we do not want to discontinue cups-browsed, but take it into the New Architecture of printing, giving it the appropriate modern interface. Currently cups-browsed discovers printers via DNS-SD, and then creates (or not creates) local print queues pointing to them according to the rules in its configuration file. But currently it creates classic CUPS queues, with PPD files generated according to the printer's IPP attributes. What we need is make it working with CUPS 3.x, which drops PPD files and classic printer drivers. For this we want tom turn it into a Printer Application, the new printer driver format, emulating a driverless IPP printer. This way CUPS can access the printers created by cups-browsed and create temporary queues for them on-demand. Internally we define with configuration file what these queues should do: Clusters, retro-fit to old CUPS, … The contributor's task is to implement this transition, using PAPPL for all standard elements of a Printer Application, like daemon, IPP parser, web admin interface, … They will make cups-browsed create a queue in the Printer Application if appropriate destination printers get discovered, and remove it when these printers disappear (turned off, user leaves network, …). CUPS will simply pick up on the emulated IPP printers then. And there will be a web interface to be created, for the configuration of the clusters, filter rules, …. one does not need to edit cups-browsed.conf manually any more.\"\n",
        "\n",
        "text_34 = \"Gutenprint is a high-quality printer driver for a wide range of inkjets, especially Epson and Canon, dye-sublimation printers and even monochrome PCL laser printers. It does not only cover many printers to give them support under Linux and free software operating systems at all, but also is optimized for highest possible print quality, so that at least on some printers and with the right settings you can even get better print quality than with the original (Windows/Mac) drivers. Gutenprint is usually used as classic CUPS driver with a CUPS filter and a PPD file generator. As, as mentioned above, CUPS will not support PPD files any more from version 3.x on and when using the CUPS Snap one cannot install PPD-based drivers already now. So a Printer Application of Gutenprint is needed. There is already one, but it is a retro-fit of the classic CUPS driver. The Printer Application simply calls the PPD generator and the filter at the right places to do its job. As Gutenprint contains all its printer support and printer capability info in libgutenprint or in files which are read by libgutenprint, the PPD generator and the filter only containing calls of functions in libgutenprint, it should be easy to create a PAPPL-based, native Printer Application for Gutenprint. Here on an incoming get-printer-attributes IPP request we call the same functions which the PPD generator calls, but instead of translating the responses into a PPD file we translate it into the IPP answer for the get-printer-attributes request. And when we have a job to print, we call the library functions which the filter calls, but directly. This does not only save us from resource-consuming calls of external executables but we are also no harnessed by the PPD file syntax and so have more flexibility in the UI representations of the (often more than 100) printer-specific options. Also, generally we should completely do away with the PPDs. Retro-fitting is only an ugly interim solution or for drivers which are not actively maintained anymore and for printers we do not have at hand and so cannot test the drivers.\"\n",
        "\n",
        "# The below texts are Case studies used in a research papper to test the accuracy of similler USG tool:\n",
        "\n",
        "text_35 = \"The Payroll Administrator maintains employee information. The Payroll Administrator is responsible for adding new employees, deleting employees and changing all employee information such as title, address, and payment classification (hourly, salaried, commissioned), as well as running administrative reports.\"\n",
        "\n",
        "text_36 = \"First, I want the website to be a place for the local scene. Somewhere the kids can come and check out upcoming events—surf competitions, lessons, things like that. Second, I need a place to sell merchandise. Boards, wet suits, clothes, videos, and things like that. But it’s gotta be easy to use and look really good. Third, I’ve always wanted a webcam pointing at the beach. This way, you don’t have to come down to check out the conditions. You can just open your laptop, go to the website, and see whether it’s worth getting up. This also means the website has to be fast.\"\n",
        "\n",
        "text_37 = \"The user of the application is able to track his/her performance when running or riding his/her bike via the GPS. His/her performance can be saved to his/her account and shared with other friends from his/her social networks. The user cannot delete any entries, once they are saved to the account. The user has the ability to create a report with all the activities by date range, or by type (running or biking).\"\n",
        "\n",
        "text_38 = \"The bakery system will maintain data from customers, products, service supplier, employees as well as generate administrative reports. The maintenance feature includes adding, deleting, and updating all customer, users and employees information that will be: name, address, date of birth and for employees payment classification too. And for the users of the bakery system, it will be saved the personal information, an id and password too. The products maintenance feature will be about to create product in the system with information such as name, value for sale, date of sale, value of purchase, quantity, value off when applicable. This feature will include, deleting, and updating the product information as well. The administrative reports will generate data from sales, crossing information with the date and value of sales. It will also include an alert function when items and products are bellow the minimum amount defined by administrative users. Finally, the system also provides a cash flow that will be the feature that provides information for administrative reports.\" "
      ],
      "metadata": {
        "id": "wmvwmxWAETCR"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coreference Resolution Using StanfordCoreNLP"
      ],
      "metadata": {
        "id": "-oiF5QdykAec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A StanfordCoreNLP object is instantiated, which starts the CoreNLP server. The path to the CoreNLP directory is passed as an argument to the constructor.\n",
        "# Start the Stanford CoreNLP server\n",
        "stanford_nlp = StanfordCoreNLP(os.path.join(os.getcwd(), 'stanford-corenlp-full-2018-10-05'))\n",
        "\n",
        "# Define the input text\n",
        "text = text_35\n",
        "\n",
        "\n",
        "# The CoreNLP pipeline is set up using a Python dictionary. The coref annotator is used for coreference resolution, and the pipelineLanguage parameter is set to en for English. The outputFormat parameter is set to json so that the output can be easily parsed.\n",
        "# Set up the CoreNLP pipeline\n",
        "props = {\n",
        "    'annotators': 'coref',\n",
        "    'pipelineLanguage': 'en',\n",
        "    'outputFormat': 'json'\n",
        "}\n",
        "\n",
        "# The StanfordCoreNLP.annotate() method is used to perform coreference resolution on the input text. The text and the pipeline configuration are passed as arguments. The output is returned as a JSON-formatted string.\n",
        "# The output from the CoreNLP server is a JSON-formatted string, which is parsed into a Python dictionary using the json.loads() method. The resulting dictionary contains information about the input text and its coreferences.\n",
        "# Perform coreference resolution\n",
        "result = stanford_nlp.annotate(text, properties=props)\n",
        "result = json.loads(result)\n",
        "\n",
        "# To replace each pronoun with its corresponding entity, the script loops over the corefs key in the resulting dictionary. For each coreference cluster, the representative mention is determined and all mentions in the cluster are replaced with it.\n",
        "# Replace pronouns with their corresponding entities\n",
        "# The first line of the code initializes a loop over all the coreference chains returned by Stanford CoreNLP\n",
        "for coref in result['corefs']:\n",
        "    # For each coreference chain, the code sorts the mentions in the chain by their start index\n",
        "    # The idea here is to replace each pronoun with the noun phrase that occurs earlier in the text. Sorting the mentions by their start index ensures that we always replace a pronoun with the closest noun phrase that it refers to.\n",
        "    mentions = sorted(result['corefs'][coref], key=lambda x: x['startIndex'])\n",
        "    # The code then initializes a variable called replace_with to the text of the first mention in the sorted list\n",
        "    # This variable will hold the final noun phrase that the pronoun will be replaced with.\n",
        "    replace_with = mentions[0]['text']\n",
        "    # The code then loops over the mentions in the chain and checks if the current mention is the representative mention\n",
        "    # The representative mention is the mention that represents the entire chain. For example, in the sentence \"John saw Mary in the park. He waved at her.\", the representative mention of the pronoun \"her\" would be \"Mary\". The code updates the replace_with variable to hold the text of the representative mention if the current mention is the representative mention.\n",
        "    for mention in mentions:\n",
        "        if mention['isRepresentativeMention']:\n",
        "            replace_with = mention['text']\n",
        "            break\n",
        "    # Finally, the code loops over all the tokens in the input text and checks if the current token is a pronoun that refers to the current coreference chain\n",
        "    for sentence in result['sentences']:\n",
        "        for token in sentence['tokens']:\n",
        "            # If the current token is a pronoun that refers to the current coreference chain, the code updates the token's word attribute to hold the text of the noun phrase that the pronoun refers to.\n",
        "            if token['originalText'] in mentions[-1]['text'] and token['pos'] == 'PRP':\n",
        "                token['word'] = replace_with\n",
        "\n",
        "# Finally, the updated text is assembled by looping over the sentences key in the resulting dictionary, joining the tokens back into text, and adding the appropriate punctuation.\n",
        "# Join the tokens back into text\n",
        "new_text = ''\n",
        "for sentence in result['sentences']:\n",
        "    for token in sentence['tokens']:\n",
        "        new_text += token['word'] + ' '\n",
        "    new_text = new_text[:-1]\n",
        "\n",
        "# The end result is a modified version of the input text where all the pronouns have been replaced with the corresponding noun phrases that they refer to.\n",
        "# Print the updated text\n",
        "print(new_text)\n",
        "\n",
        "# Stop the Stanford CoreNLP server\n",
        "stanford_nlp.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGttMuoEjxPa",
        "outputId": "c374b83e-61c8-4832-8674-87d2316a4fa0"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Payroll Administrator maintains employee information .The Payroll Administrator is responsible for adding new employees , deleting employees and changing all employee information such as title , address , and payment classification -LRB- hourly , salaried , commissioned -RRB- , as well as running administrative reports .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_text = new_text.replace(\" .\", \". \").replace(\" ,\", \",\").replace(\"-LRB- \", \"(\").replace(\" -RRB-\", \")\")\n",
        "print(final_text)"
      ],
      "metadata": {
        "id": "JiEP5w23zCDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b48b3fba-2245-4fc8-d6dc-32f9a3e4b993"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Payroll Administrator maintains employee information. The Payroll Administrator is responsible for adding new employees, deleting employees and changing all employee information such as title, address, and payment classification (hourly, salaried, commissioned), as well as running administrative reports. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rule Based User Stories Generation Using Spacy "
      ],
      "metadata": {
        "id": "4gNGAA2Sk6CM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "\n",
        "# put final_text if you want to use Coreference Resolution or if you want to skip using Coreference Resolution, you could put any of the (text_1 ---> text_38) instead.\n",
        "doc = nlp(final_text)\n",
        "\n",
        "\n",
        "sentences_list = list(doc.sents)\n",
        "\n",
        "for sentence in sentences_list:\n",
        "  print(\"Sentence: {}\".format(sentence))\n",
        "\n",
        "  subject_list = []\n",
        "  verb_list = []\n",
        "  object_list = []\n",
        "  subject_verb_object_list = []\n",
        "  user_stories_components_list = []\n",
        "  user_stories_list = []\n",
        "  \n",
        "  for token in sentence:\n",
        "    get_subject_verb_object_from_sentence(token, subject_list, verb_list, object_list)\n",
        "\n",
        "\n",
        "  create_subject_verb_object_list(doc, sentence, subject_list, verb_list, object_list, subject_verb_object_list)\n",
        "\n",
        "  validate_subject_verb_object_list(doc, sentence, subject_list, verb_list, object_list, subject_verb_object_list, user_stories_components_list)\n",
        "\n",
        "  generate_user_stories_from_sentence(doc, sentence, subject_verb_object_list, user_stories_components_list, user_stories_list)\n",
        "\n",
        "  print(\"______________________________________________________________________________________________\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ao2_W5d2VTo",
        "outputId": "230c82c7-6406-4527-c0b5-909582e45c16"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: The Payroll Administrator maintains employee information.\n",
            "subject_verb_object_list: [{'subject': Administrator, 'verb': maintains, 'object': information, 'isNegative_sentence': False}]\n",
            "user_stories_components_list: [{'subject': 'the payroll administrator', 'pronoun': 'I', 'verb': 'maintain', 'verb_in_title': 'maintain', 'object': 'employee information', 'object_in_title': 'employee information', 'isNegative': False}]\n",
            "user_stories_list: [{'userStoryTitle': 'Maintain Employee Information', 'userStoryDescription': 'As the payroll administrator, I want to be able to maintain employee information.', 'userStoryID': UUID('9cf208ca-057a-445d-a39e-34e35edf5910')}]\n",
            "______________________________________________________________________________________________\n",
            "Sentence: The Payroll Administrator is responsible for adding new employees, deleting employees and changing all employee information such as title, address, and payment classification (hourly, salaried, commissioned), as well as running administrative reports.\n",
            "subject_verb_object_list: [{'subject': Administrator, 'verb': adding, 'object': employees, 'isNegative_sentence': False}, {'subject': Administrator, 'verb': deleting, 'object': employees, 'isNegative_sentence': False}, {'subject': Administrator, 'verb': changing, 'object': information, 'isNegative_sentence': False}, {'subject': Administrator, 'verb': running, 'object': reports, 'isNegative_sentence': False}]\n",
            "user_stories_components_list: [{'subject': 'the payroll administrator', 'pronoun': 'I', 'verb': 'add', 'verb_in_title': 'add', 'object': 'new employees', 'object_in_title': 'new employees', 'isNegative': False}, {'subject': 'the payroll administrator', 'pronoun': 'I', 'verb': 'delete', 'verb_in_title': 'delete', 'object': 'employees', 'object_in_title': 'employees', 'isNegative': False}, {'subject': 'the payroll administrator', 'pronoun': 'I', 'verb': 'change', 'verb_in_title': 'change', 'object': 'all employee information such as title, address, and payment classification (hourly, salaried, commissioned)', 'object_in_title': 'all employee information such as title, address, and payment classification (hourly, salaried, commissioned)', 'isNegative': False}, {'subject': 'the payroll administrator', 'pronoun': 'I', 'verb': 'run', 'verb_in_title': 'run', 'object': 'administrative reports', 'object_in_title': 'administrative reports', 'isNegative': False}]\n",
            "user_stories_list: [{'userStoryTitle': 'Add New Employees', 'userStoryDescription': 'As the payroll administrator, I want to be able to add new employees.', 'userStoryID': UUID('13e6512c-ecc3-4859-abb7-788caad594a7')}, {'userStoryTitle': 'Delete Employees', 'userStoryDescription': 'As the payroll administrator, I want to be able to delete employees.', 'userStoryID': UUID('1297230f-b4e4-4881-83dc-ea563c861c9d')}, {'userStoryTitle': 'Change All Employee Information Such As Title, Address, And Payment Classification (Hourly, Salaried, Commissioned)', 'userStoryDescription': 'As the payroll administrator, I want to be able to change all employee information such as title, address, and payment classification (hourly, salaried, commissioned).', 'userStoryID': UUID('d359fd94-bcce-4127-8ebc-e9ae483fb7d3')}, {'userStoryTitle': 'Run Administrative Reports', 'userStoryDescription': 'As the payroll administrator, I want to be able to run administrative reports.', 'userStoryID': UUID('1d42ed14-1a2f-4840-a7bb-a2cc1914c4d6')}]\n",
            "______________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experminting "
      ],
      "metadata": {
        "id": "CHB25jzuN1m1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#text = \"The Payroll Administrator maintains employee information. The Payroll Administrator is responsible for adding new employees, deleting employees and changing all employee information such as title, address, and payment classification (hourly, salaried, commissioned), as well as running administrative reports.\"\n",
        "\n",
        "#text = \"The product catalog should be easy to navigate and allow users to filter products by different criteria such as price range, product type, and brand. The shopping cart should display the products added by the user and the total price. The checkout process should be simple and secure, and users should be able to provide their name, shipping address, and payment method easily. We also identified the need for a payment gateway to ensure secure payments. Lastly, users should be able to view their order history, change their details, and manage their account from a user profile page. To capture the needs of the users, we created user stories that reflect the requirements. These stories include browsing and filtering products, adding products to the shopping cart, checking out easily and securely, receiving email confirmation of orders, and viewing order history from a user profile page. We will use these user stories to guide the development of the website and ensure that all the requirements are met. Overall, we want to create a website that meets the needs of our users and provides them with a seamless shopping experience. We look forward to working together to bring this project to life.\"\n",
        "\n",
        "#text = \"Overall, we want to create a website that meets the needs of our users and provides them with a seamless shopping experience.\"\n",
        "\n",
        "#text = \"Finding and classifying repetitive DNA sequence in eukaryotic genomes is both an important first step ahead of further genome annotation, and also interesting in its own right as repeats frequently drive genome evolution. Repeats in DNA can be broken into a number of different major classes such as LINEs, SINEs and LTRs. Global biodiversity efforts such as Darwin Tree of Life, the European Reference Genome Atlas and the Earth BioGenome Project are producing hundreds and soon thousands of high-quality reference genomes, that will all need repeat annotation. Currently we have two potential approach to annotating repeats. The first is building a repeat library for a species (using RepeatModeler) and then annotating the repeats on the genome (using RepeatMasker). This method both finds and classifies the repeats and finds lineage specific repeats, however building a repeat library is computationally costly. The second approach is to use an extremely fast k-mer approach (REpeatDetector, aka Red), to mask the genome in a fraction of this time. The downside is that this approach does not classify repeats and so is not very informative for researchers studying repeat evolution. In this project we want to explore Deep Learning in order to help classify repeats. We have large existing training sets across hundreds of species, spanning billions of classified repeats. As part of this projects you would train a neural network to take as input an unclassified repeat sequence and label it according to the class of repeats it belongs. You will explore the most efficient approach in terms of both preparing the training data and constructing the network. If the training is successful, we will then test the resulting model from a perspective of compute efficiency, i.e. does the model produce similar results to our existing method of classification (i.e. building a repeat library for the species and then using it to find and classify repeats) and what is the relative compute cost in each approach. Depending on the success and progress related to the above, there may also be the opportunity to take the project a step further, in terms of generative repeat library construction, i.e. given a fast k-mer derived set of repeat sequences and their coordinates on the genome, is it possible to generate a repeat library. This would be highly experimental and only considered after fast and excellent progress on the core project.\"\n",
        "\n",
        "#text = \"The Payroll Administrator maintains employee information. He is responsible for adding new employees, deleting employees and changing all employee information such as title, address, and payment classification (hourly, salaried, commissioned), as well as running administrative reports.\"\n",
        "\n",
        "#text = \"The Payroll Administrator maintains employee information for example such as not impinfo and not imp. The Payroll Administrator is responsible for adding new employees such as addnewmp and addoldemp, deleting employees such as delnewmp and deloldemp, and changing all employee information such as title, address, and payment classification (hourly, salaried, commissioned), as well as running administrative reports such as imprepo and not impreports.\"\n",
        "\n",
        "#text = \"The Payroll Administrator is not responsible for adding new employees, deleting employees and changing all employee information such as title, address, and payment classification (hourly, salaried, commissioned), as well as running administrative reports.\"\n",
        "\n",
        "#text = \"The Payroll Administrator maintains employee information for example such as not impinfo and not imp. The Payroll Administrator is responsible for adding new employees such as addnewmp and addoldemp, deleting employees such as delnewmp and deloldemp, and changing all employee information such as title, address, and payment classification (hourly, salaried, commissioned), as well as running administrative reports such as imprepo and not impreports.\"\n",
        "\n",
        "#text = \"The product catalog should be easy to navigate and allow users to filter products by different criteria such as price range, product type, and brand.\"\n",
        "\n",
        "#text = \"The product catalog should be easy to navigate and allow users to filter products by different criteria such as price range, product type, and brand. The shopping cart should display the products added by the user and the total price. The checkout process should be simple and secure, and users should be able to provide their name, shipping address, and payment method easily. We also identified the need for a payment gateway to ensure secure payments. Lastly, users should be able to view their order history, change their details, and manage their account from a user profile page. To capture the needs of the users, we created user stories that reflect the requirements. These stories include browsing and filtering products, adding products to the shopping cart, checking out easily and securely, receiving email confirmation of orders, and viewing order history from a user profile page. We will use these user stories to guide the development of the website and ensure that all the requirements are met. Overall, we want to create a website that meets the needs of our users and provides them with a seamless shopping experience. We look forward to working together to bring this project to life.\"\n",
        "\n",
        "#text = \"The employee information is maintained by the Payroll Administrator.\"\n",
        "\n",
        "#text = \"Repeats in DNA can be broken into a number of different major classes such as LINEs, SINEs and LTRs.\"\n",
        "\n",
        "#Sentence = \"This method is responsible for both finding and classifing the repeats and finding lineage specific repeats, however building a repeat library is computationally costly.\"\n",
        "\n",
        "#Sentence = \"Finding and classifying repetitive DNA sequence in eukaryotic genomes is both an important first step ahead of further genome annotation, and also interesting in its own right as repeats frequently drive genome evolution.\"\n",
        "\n",
        "#Sentence = \"The downside is not very informative for researchers studying repeat evolution.\"\n",
        "\n",
        "#text = \"The Payroll Administrator is responsible for adding new employees such as addnewmp and addoldemp, deleting employees such as delnewmp and deloldemp, and changing all employee information such as title, address, and payment classification (hourly, salaried, commissioned), as well as running administrative reports such as imprepo and not impreports.\"\n",
        "\n",
        "#Sentence = \" As part of this projects you would train a neural network to take as input an unclassified repeat sequence and label a neural network according to the class of repeats a neural network belongs.\""
      ],
      "metadata": {
        "id": "P8MWXmmj9zNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Hello, (world)! []{}<>\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.is_left_punct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5RvvqiPZCS6",
        "outputId": "7d674df9-783f-48bc-98fa-8b1c3c252634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello False\n",
            ", False\n",
            "( True\n",
            "world False\n",
            ") False\n",
            "! False\n",
            "[ True\n",
            "] False\n",
            "{ True\n",
            "} False\n",
            "< True\n",
            "> False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Sentence = \" As part of this projects you would train a neural network to take as input an unclassified repeat sequence and label a neural network according to the class of repeats a neural network belongs.\"\n",
        "\n",
        "doc = nlp(Sentence)\n",
        "\n",
        "\n",
        "print('{:<12}{:<10}{:<10}{:<10}'.format('token_text', 'token_pos', 'token_dep', 'token_lemm'))\n",
        "\n",
        "for token in doc:\n",
        "    # Get the token text, part-of-speech tag and dependency label\n",
        "    token_text = token.text\n",
        "    token_pos = token.pos_\n",
        "    token_dep = token.dep_\n",
        "    token_lemm = token.lemma_\n",
        "    # This is for formatting only\n",
        "    print('{:<12}{:<10}{:<10}{:<10}'.format(token_text, token_pos, token_dep, token_lemm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV6rMh7R1H2-",
        "outputId": "c5b5578c-9a11-4a06-9dac-08931f6d5dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_text  token_pos token_dep token_lemm\n",
            "            SPACE     dep                 \n",
            "As          ADP       prep      as        \n",
            "part        NOUN      pobj      part      \n",
            "of          ADP       prep      of        \n",
            "this        DET       det       this      \n",
            "projects    NOUN      pobj      project   \n",
            "you         PRON      nsubj     you       \n",
            "would       AUX       aux       would     \n",
            "train       VERB      ROOT      train     \n",
            "a           DET       det       a         \n",
            "neural      ADJ       amod      neural    \n",
            "network     NOUN      dobj      network   \n",
            "to          PART      aux       to        \n",
            "take        VERB      xcomp     take      \n",
            "as          ADP       prep      as        \n",
            "input       NOUN      pobj      input     \n",
            "an          DET       det       an        \n",
            "unclassifiedADJ       amod      unclassified\n",
            "repeat      NOUN      compound  repeat    \n",
            "sequence    NOUN      dobj      sequence  \n",
            "and         CCONJ     cc        and       \n",
            "label       VERB      conj      label     \n",
            "a           DET       det       a         \n",
            "neural      ADJ       amod      neural    \n",
            "network     NOUN      dobj      network   \n",
            "according   VERB      prep      accord    \n",
            "to          ADP       prep      to        \n",
            "the         DET       det       the       \n",
            "class       NOUN      pobj      class     \n",
            "of          ADP       prep      of        \n",
            "repeats     NOUN      pobj      repeat    \n",
            "a           DET       det       a         \n",
            "neural      ADJ       amod      neural    \n",
            "network     NOUN      nsubj     network   \n",
            "belongs     VERB      ccomp     belong    \n",
            ".           PUNCT     punct     .         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Sentence = \" As part of this projects you would train a neural network to take as input an unclassified repeat sequence and label a neural network according to the class of repeats a neural network belongs.\"\n",
        "\n",
        "doc = nlp(Sentence)\n",
        "\n",
        "for token in doc:\n",
        "  if(token.pos_ == \"ADP\"):\n",
        "    print(token)\n",
        "    # print(list(token.ancestors))\n",
        "    # print(list(token.subtree))\n",
        "    print(list(token.sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-dAAZVY4oWV",
        "outputId": "f8bf07d2-8bf6-4893-c5f7-02b591f45cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As\n",
            "[ , As, part, of, this, projects, you, would, train, a, neural, network, to, take, as, input, an, unclassified, repeat, sequence, and, label, a, neural, network, according, to, the, class, of, repeats, a, neural, network, belongs, .]\n",
            "of\n",
            "[ , As, part, of, this, projects, you, would, train, a, neural, network, to, take, as, input, an, unclassified, repeat, sequence, and, label, a, neural, network, according, to, the, class, of, repeats, a, neural, network, belongs, .]\n",
            "as\n",
            "[ , As, part, of, this, projects, you, would, train, a, neural, network, to, take, as, input, an, unclassified, repeat, sequence, and, label, a, neural, network, according, to, the, class, of, repeats, a, neural, network, belongs, .]\n",
            "to\n",
            "[ , As, part, of, this, projects, you, would, train, a, neural, network, to, take, as, input, an, unclassified, repeat, sequence, and, label, a, neural, network, according, to, the, class, of, repeats, a, neural, network, belongs, .]\n",
            "of\n",
            "[ , As, part, of, this, projects, you, would, train, a, neural, network, to, take, as, input, an, unclassified, repeat, sequence, and, label, a, neural, network, according, to, the, class, of, repeats, a, neural, network, belongs, .]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Sentence = \" As part of this projects you would train a neural network to take as input an unclassified repeat sequence and label a neural network according to the class of repeats a neural network belongs.\"\n",
        "\n",
        "doc = nlp(Sentence)\n",
        "\n",
        "for token in doc:\n",
        "  if(token.pos_ == \"VERB\"):\n",
        "    print(token)\n",
        "    print(list(token.subtree))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp4UC4noID1b",
        "outputId": "7ab6a6ed-baf6-48be-e4e7-0d546df7d666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "[ , As, part, of, this, projects, you, would, train, a, neural, network, to, take, as, input, an, unclassified, repeat, sequence, and, label, a, neural, network, according, to, the, class, of, repeats, a, neural, network, belongs, .]\n",
            "take\n",
            "[to, take, as, input, an, unclassified, repeat, sequence, and, label, a, neural, network, according, to, the, class, of, repeats, a, neural, network, belongs]\n",
            "label\n",
            "[label, a, neural, network, according, to, the, class, of, repeats]\n",
            "according\n",
            "[according, to, the, class, of, repeats]\n",
            "belongs\n",
            "[a, neural, network, belongs]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_list = [1, 2, 3, 4, 3, 5, 3]\n",
        "item = 3\n",
        "\n",
        "indices = [index for index in range(len(my_list)) if my_list[index] == item]\n",
        "last_index = indices[-1]\n",
        "\n",
        "print(last_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNAkE6ITa0C1",
        "outputId": "fa94f562-e4f5-4a09-9d6d-ea74f5822114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"i love (rozy\"\n",
        "\n",
        "print(text[-1])\n",
        "\n",
        "print(text + \")\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XckiJNBtHU93",
        "outputId": "345b7c92-ccbb-4160-962d-b54cccc14e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y\n",
            "i love (rozy)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_list = [1,2,3,4,5,6,7]\n",
        "index_of_4 = my_list.index(4)\n",
        "\n",
        "for i in my_list[index_of_4:]:\n",
        "  print(i)\n",
        "\n",
        "print(\"_____________________\")\n",
        "\n",
        "for i in my_list[:index_of_4+1][::-1]:\n",
        "  print(i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjHFhjvN82ou",
        "outputId": "da8c83fb-d9c8-4add-d0b9-3ce4a1a1aeee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "_____________________\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_list = [1, 2, 3, 4, 5, 6]\n",
        "\n",
        "my_list_reversed = my_list[::-1]\n",
        "\n",
        "print(my_list[3:])\n",
        "print(my_list_reversed[:3])\n",
        "\n",
        "# print(my_list[0:3])\n",
        "# print(my_list[::-1])\n",
        "# print(my_list[2::-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqHBDwrUEBtT",
        "outputId": "46fefd43-034b-47cd-b0b4-6a9ea07c72fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 5, 6]\n",
            "[6, 5, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text =  \"The book was written by the author\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "\n",
        "print('{:<12}{:<10}{:<10}{:<10}'.format('token_text', 'token_pos', 'token_dep', 'token_lemm'))\n",
        "\n",
        "for token in doc:\n",
        "    # Get the token text, part-of-speech tag and dependency label\n",
        "    token_text = token.text\n",
        "    token_pos = token.pos_\n",
        "    token_dep = token.dep_\n",
        "    token_lemm = token.lemma_\n",
        "    # This is for formatting only\n",
        "    print('{:<12}{:<10}{:<10}{:<10}'.format(token_text, token_pos, token_dep, token_lemm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UCQgNRgqXde",
        "outputId": "d1cb87ac-43ca-43b9-a9b3-c19a74e102fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_text  token_pos token_dep token_lemm\n",
            "The         DET       det       the       \n",
            "book        NOUN      nsubjpass book      \n",
            "was         AUX       auxpass   be        \n",
            "written     VERB      ROOT      write     \n",
            "by          ADP       agent     by        \n",
            "the         DET       det       the       \n",
            "author      NOUN      pobj      author    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The Payroll Administrator maintains employee information for example such as not impinfo and not imp.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "\n",
        "print('{:<12}{:<10}{:<10}{:<10}'.format('token_text', 'token_pos', 'token_dep', 'token_lemm'))\n",
        "\n",
        "for token in doc:\n",
        "    # Get the token text, part-of-speech tag and dependency label\n",
        "    token_text = token.text\n",
        "    token_pos = token.pos_\n",
        "    token_dep = token.dep_\n",
        "    token_lemm = token.lemma_\n",
        "    # This is for formatting only\n",
        "    print('{:<12}{:<10}{:<10}{:<10}'.format(token_text, token_pos, token_dep, token_lemm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc9j5R8ozOlz",
        "outputId": "dc79c48b-6f9b-4f0f-b155-53018ef2828b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_text  token_pos token_dep token_lemm\n",
            "The         DET       det       the       \n",
            "Payroll     PROPN     compound  Payroll   \n",
            "AdministratorPROPN     nsubj     Administrator\n",
            "maintains   VERB      ROOT      maintain  \n",
            "employee    NOUN      compound  employee  \n",
            "information NOUN      dobj      information\n",
            "for         ADP       prep      for       \n",
            "example     NOUN      pobj      example   \n",
            "such        ADJ       amod      such      \n",
            "as          ADP       prep      as        \n",
            "not         PART      neg       not       \n",
            "impinfo     NOUN      pobj      impinfo   \n",
            "and         CCONJ     cc        and       \n",
            "not         PART      neg       not       \n",
            "imp         ADJ       conj      imp       \n",
            ".           PUNCT     punct     .         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The Payroll Administrator maintains employee information for example such as not impinfo and not imp.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "  if(token.pos_ == \"ADP\"):\n",
        "    print(token)\n",
        "    print(list(token.subtree))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD6aHJZrqX2d",
        "outputId": "afc66cf0-d364-4724-992c-23e8cadc9e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for\n",
            "[for, example, such, as, not, impinfo, and, not, imp]\n",
            "as\n",
            "[such, as, not, impinfo, and, not, imp]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The Payroll Administrator is not responsible for adding new employees, deleting employees and changing all employee information such as title, address, and payment classification (hourly, salaried, commissioned), as well as running administrative reports.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "  if(token.lemma_ == \"be\"):\n",
        "    print(token)\n",
        "    print(list(token.children))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J2bAY99EGu-",
        "outputId": "25a70d65-5739-4a39-a9eb-2f4e96dd1d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is\n",
            "[Administrator, not, responsible, .]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Sentence = \"Finding and classifying repetitive DNA sequence in eukaryotic genomes is both an important first step ahead of further genome annotation, and also interesting in its own right as repeats frequently drive genome evolution.\"\n",
        "\n",
        "doc = nlp(Sentence)\n",
        "\n",
        "for token in doc:\n",
        "  if(token.pos_ == \"VERB\"):\n",
        "    print(token)\n",
        "    print(list(token.conjuncts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xej-U-gKtnD7",
        "outputId": "15e2596e-5892-4881-be76-743d8d52ca90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding\n",
            "[classifying]\n",
            "classifying\n",
            "[Finding]\n",
            "drive\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Sentence = text_1\n",
        "\n",
        "doc = nlp(Sentence)\n",
        "\n",
        "for token in doc:\n",
        "  if(token.pos_ == \"VERB\"):\n",
        "    print(token)\n",
        "    print(list(token.subtree))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzaIXLF8xVQu",
        "outputId": "df001848-16ae-41b6-ad84-939be479eeb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding\n",
            "[Finding, and, classifying, repetitive, DNA, sequence, in, eukaryotic, genomes]\n",
            "classifying\n",
            "[classifying, repetitive, DNA, sequence, in, eukaryotic, genomes]\n",
            "drive\n",
            "[both, an, important, first, step, ahead, of, further, genome, annotation, ,, and, also, interesting, in, its, own, right, as, repeats, frequently, drive, genome, evolution]\n",
            "broken\n",
            "[Repeats, in, DNA, can, be, broken, into, a, number, of, different, major, classes, such, as, LINEs, ,, SINEs, and, LTRs, .]\n",
            "producing\n",
            "[Global, biodiversity, efforts, such, as, Darwin, Tree, of, Life, ,, the, European, Reference, Genome, Atlas, and, the, Earth, BioGenome, Project, are, producing, hundreds, and, soon, thousands, of, high, -, quality, reference, genomes]\n",
            "need\n",
            "[Global, biodiversity, efforts, such, as, Darwin, Tree, of, Life, ,, the, European, Reference, Genome, Atlas, and, the, Earth, BioGenome, Project, are, producing, hundreds, and, soon, thousands, of, high, -, quality, reference, genomes, ,, that, will, all, need, repeat, annotation, .]\n",
            "have\n",
            "[Currently, we, have, two, potential, approach, to, annotating, repeats, .]\n",
            "building\n",
            "[The, first, is, building, a, repeat, library, for, a, species, (, using, RepeatModeler, ), and, then, annotating, the, repeats, on, the, genome, (, using, RepeatMasker, ), .]\n",
            "using\n",
            "[using, RepeatModeler]\n",
            "annotating\n",
            "[then, annotating, the, repeats, on, the, genome, (, using, RepeatMasker, )]\n",
            "using\n",
            "[using, RepeatMasker]\n",
            "finds\n",
            "[This, method, both, finds, and, classifies, the, repeats, and, finds, lineage, specific, repeats, ,, however, building, a, repeat, library, is, computationally, costly, .]\n",
            "classifies\n",
            "[classifies, the, repeats, and, finds, lineage, specific, repeats, ,]\n",
            "finds\n",
            "[finds, lineage, specific, repeats, ,]\n",
            "building\n",
            "[however, building, a, repeat, library]\n",
            "use\n",
            "[to, use, an, extremely, fast, k, -, mer, approach, (, REpeatDetector, ,, aka, Red, ), ,, to, mask, the, genome, in, a, fraction, of, this, time]\n",
            "mask\n",
            "[to, mask, the, genome, in, a, fraction, of, this, time]\n",
            "classify\n",
            "[that, this, approach, does, not, classify, repeats, and, so, is, not, very, informative, for, researchers, studying, repeat, evolution]\n",
            "studying\n",
            "[studying, repeat, evolution]\n",
            "want\n",
            "[In, this, project, we, want, to, explore, Deep, Learning, in, order, to, help, classify, repeats, .]\n",
            "explore\n",
            "[to, explore, Deep, Learning, in, order, to, help, classify, repeats]\n",
            "help\n",
            "[to, help, classify, repeats]\n",
            "classify\n",
            "[classify]\n",
            "have\n",
            "[We, have, large, existing, training, sets, across, hundreds, of, species, ,, spanning, billions, of, classified, repeats, .]\n",
            "existing\n",
            "[existing]\n",
            "spanning\n",
            "[spanning, billions, of, classified, repeats]\n",
            "train\n",
            "[As, part, of, this, projects, you, would, train, a, neural, network, to, take, as, input, an, unclassified, repeat, sequence, and, label, it, according, to, the, class, of, repeats, it, belongs, .]\n",
            "take\n",
            "[to, take, as, input, an, unclassified, repeat, sequence, and, label, it, according, to, the, class, of, repeats, it, belongs]\n",
            "label\n",
            "[label, it, according, to, the, class, of, repeats, it, belongs]\n",
            "according\n",
            "[according, to, the, class, of, repeats, it, belongs]\n",
            "belongs\n",
            "[it, belongs]\n",
            "explore\n",
            "[You, will, explore, the, most, efficient, approach, in, terms, of, both, preparing, the, training, data, and, constructing, the, network, .]\n",
            "preparing\n",
            "[preparing, the, training, data, and, constructing, the, network]\n",
            "constructing\n",
            "[constructing, the, network]\n",
            "test\n",
            "[If, the, training, is, successful, ,, we, will, then, test, the, resulting, model, from, a, perspective, of, compute, efficiency]\n",
            "resulting\n",
            "[resulting]\n",
            "produce\n",
            "[If, the, training, is, successful, ,, we, will, then, test, the, resulting, model, from, a, perspective, of, compute, efficiency, ,, i.e., does, the, model, produce, similar, results, to, our, existing, method, of, classification, (, i.e., building, a, repeat, library, for, the, species, and, then, using, it, to, find, and, classify, repeats, ), and, what, is, the, relative, compute, cost, in, each, approach, .]\n",
            "existing\n",
            "[existing]\n",
            "building\n",
            "[(, i.e., building, a, repeat, library, for, the, species, and, then, using, it, to, find, and, classify, repeats, )]\n",
            "using\n",
            "[then, using, it, to, find, and, classify, repeats]\n",
            "find\n",
            "[to, find, and, classify]\n",
            "classify\n",
            "[classify]\n",
            "Depending\n",
            "[Depending, on, the, success, and, progress, related, to, the, above]\n",
            "related\n",
            "[related, to, the, above]\n",
            "take\n",
            "[to, take, the, project, a, step, further, ,, in, terms, of, generative, repeat, library, construction, ,]\n",
            "given\n",
            "[i.e., given, a, fast, k, -, mer, derived, set, of, repeat, sequences, and, their, coordinates, on, the, genome]\n",
            "derived\n",
            "[derived]\n",
            "generate\n",
            "[to, generate, a, repeat, library]\n",
            "considered\n",
            "[only, considered, after, fast, and, excellent, progress, on, the, core, project]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence = \"This method both finds and classifies the repeats and finds lineage specific repeats, however building a repeat library is computationally costly.\"\n",
        "text = \"The Payroll Administrator maintains employee information. The Payroll Administrator is responsible for adding new employees, deleting employees and changing all employee information such as title, address, and payment classification (hourly, salaried, commissioned), as well as running administrative reports.\"\n",
        "Sentence = \"The downside is that this approach does not classify repeats and so that is not very informative for researchers studying repeat evolution.\"\n",
        "doc = nlp(Sentence)\n",
        "\n",
        "for token in doc:\n",
        "  if(token.dep_ == \"nsubj\"):\n",
        "    print(token)\n",
        "    print(list(token.subtree))"
      ],
      "metadata": {
        "id": "Ws0basj2xVYo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26aad4d1-37f9-4a9a-c18a-060e42f20a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downside\n",
            "[The, downside]\n",
            "approach\n",
            "[this, approach]\n",
            "that\n",
            "[that]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Sentence = \"This method both finds and classifies the repeats and finds lineage specific repeats, however building a repeat library is computationally costly.\"\n",
        "doc = nlp(Sentence)\n",
        "\n",
        "token_list = [t.text.lower() for t in doc[0:6]]\n",
        "\n",
        "token_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBkT4VN8VP8c",
        "outputId": "22b2444c-66ec-4979-d4cb-7253ca58138a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'method', 'both', 'finds', 'and', 'classifies']"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aac2w2gjAruK",
        "outputId": "23560cc1-8de9-4757-8336-0bade07150b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first is building a repeat library for a species (using RepeatModeler) and then annotating the repeats on the genome (using RepeatMasker).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Sentence = \"Finding and classifying repetitive DNA sequence in eukaryotic genomes is both an important first step ahead of further genome annotation, and also interesting in its own right as repeats frequently drive genome evolution.\"\n",
        "\n",
        "doc = nlp(Sentence)\n",
        "\n",
        "print(doc[0].i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whc7iU9RqX5j",
        "outputId": "22aff52b-dadf-4871-ad5b-196768b842a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"The product catalog should be easy to navigate and allow users to filter products by different criteria such as price range, product type, and brand.\"\n",
        "\n",
        "doc = nlp(test_text)\n",
        "\n",
        "\n",
        "print('{:<12}{:<10}{:<10}{:<10}'.format('token_text', 'token_pos', 'token_dep', 'token_lemm'))\n",
        "\n",
        "for token in doc:\n",
        "    # Get the token text, part-of-speech tag and dependency label\n",
        "    token_text = token.text\n",
        "    token_pos = token.pos_\n",
        "    token_dep = token.dep_\n",
        "    token_lemm = token.lemma_\n",
        "    # This is for formatting only\n",
        "    print('{:<12}{:<10}{:<10}{:<10}'.format(token_text, token_pos, token_dep, token_lemm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwGAWGMFqrPR",
        "outputId": "8ff00ab0-afa4-4b49-d501-27d88f1babd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token_text  token_pos token_dep token_lemm\n",
            "The         DET       det       the       \n",
            "product     NOUN      compound  product   \n",
            "catalog     NOUN      nsubj     catalog   \n",
            "should      AUX       aux       should    \n",
            "be          AUX       ROOT      be        \n",
            "easy        ADJ       acomp     easy      \n",
            "to          PART      aux       to        \n",
            "navigate    VERB      xcomp     navigate  \n",
            "and         CCONJ     cc        and       \n",
            "allow       VERB      conj      allow     \n",
            "users       NOUN      nsubj     user      \n",
            "to          PART      aux       to        \n",
            "filter      VERB      ccomp     filter    \n",
            "products    NOUN      dobj      product   \n",
            "by          ADP       prep      by        \n",
            "different   ADJ       amod      different \n",
            "criteria    NOUN      pobj      criterion \n",
            "such        ADJ       amod      such      \n",
            "as          ADP       prep      as        \n",
            "price       NOUN      compound  price     \n",
            "range       NOUN      pobj      range     \n",
            ",           PUNCT     punct     ,         \n",
            "product     NOUN      compound  product   \n",
            "type        NOUN      conj      type      \n",
            ",           PUNCT     punct     ,         \n",
            "and         CCONJ     cc        and       \n",
            "brand       NOUN      conj      brand     \n",
            ".           PUNCT     punct     .         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "for item1, item2 in zip(my_list[:-1], my_list[1:]):\n",
        "    print(type(item1),item2)"
      ],
      "metadata": {
        "id": "krLt90BOqrVQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a828f9f1-31c5-4bf3-a4bc-fea36f86aae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'int'> 2\n",
            "<class 'int'> 3\n",
            "<class 'int'> 4\n",
            "<class 'int'> 5\n",
            "<class 'int'> 6\n",
            "<class 'int'> 7\n",
            "<class 'int'> 8\n",
            "<class 'int'> 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The Payroll Administrator is not responsible for adding new employees, deleting employees and changing all employee information such as title, address, and payment classification (hourly, salaried, commissioned), as well as running administrative reports.\"\n",
        "\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "sentences_list = list(doc.sents)\n",
        "\n",
        "for sentence in sentences_list:\n",
        "  for token in sentence:\n",
        "    if(token.dep_ == \"neg\"):\n",
        "      print(token, list(token.ancestors))\n",
        "  print(\"____________________________________________________\")"
      ],
      "metadata": {
        "id": "TBvHUQuPuqwW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3bb281d-83e9-494b-9804-122fd500d826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not [is]\n",
            "____________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The employee information is maintained by the Payroll Administrator.\"\n",
        "\n",
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "sentences_list = list(doc.sents)\n",
        "\n",
        "for sentence in sentences_list:\n",
        "  for token in sentence:\n",
        "    if(token.dep_ == \"pobj\"):\n",
        "      print(token, list(token.ancestors))\n",
        "  print(\"____________________________________________________\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYj0B2b02V8n",
        "outputId": "e7e4cbe1-9ccf-4872-f970-6b1785b96d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Administrator [by, maintained]\n",
            "____________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "sentences_list = list(doc.sents)\n",
        "\n",
        "for sentence in sentences_list:\n",
        "  for token in sentence:\n",
        "    if(token.dep_ == \"nsubjpass\"):\n",
        "      print(token, list(token.ancestors))\n",
        "  print(\"____________________________________________________\")"
      ],
      "metadata": {
        "id": "86ql5z_Uu6n5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e765ed1-4453-401a-f7fc-9437d711d6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "information [maintained]\n",
            "____________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "doc = nlp(final_text)\n",
        "\n",
        "sentences_list = list(doc.sents)\n",
        "\n",
        "for sentence in sentences_list:\n",
        "  verb_number = check_verb_number_in_sentence(sentence)\n",
        "  print(sentence)\n",
        "  print(verb_number)"
      ],
      "metadata": {
        "id": "zvS8p_bx5DPl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "d8f89f33-5418-41b3-e4b0-5c2656c26ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-6cfae10a3ece>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_lg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msentences_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'final_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "doc = nlp(final_text)\n",
        "\n",
        "sentences_list = list(doc.sents)\n",
        "\n",
        "for sentence in sentences_list:\n",
        "  for token in sentence:\n",
        "    if(token.pos_ == \"VERB\"):\n",
        "      print(\"Verb: {}, POS: {}\".format(token, token.pos_))\n",
        "      print(\"neighboring token: {}\".format(token.nbor()))\n",
        "      print(\"is_ancestor: {}\".format(token.is_ancestor()))\n",
        "      print(list(token.subtree))\n",
        "      raw_tokens = []\n",
        "      for item in token.subtree:\n",
        "        raw_tokens.append(item.text)\n",
        "      raw_text = \" \".join(raw_tokens)\n",
        "      # print(raw_tokens)\n",
        "      print(raw_text)\n",
        "      print(\"_________________________________________________________________________\")"
      ],
      "metadata": {
        "id": "Q2RGBPQ69gzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "doc = nlp(final_text)\n",
        "\n",
        "sentences_list = list(doc.sents)\n",
        "\n",
        "\n",
        "for sentence in sentences_list:\n",
        "  \n",
        "  print(\"sentence: {}\".format(sentence))\n",
        "\n",
        "  subjects_list = []\n",
        "  objects_list = []\n",
        "  verbs_list = []\n",
        "\n",
        "  for token in sentence:\n",
        "    if(token.dep_ == \"nsubj\"):\n",
        "      subjects_list.append(token)\n",
        "    if(token.pos_ == \"VERB\"):\n",
        "      verbs_list.append(token)\n",
        "    if(token.dep_ == \"dobj\"):\n",
        "      objects_list.append(token) \n",
        "\n",
        "  for subject in subjects_list:\n",
        "    for verb in verbs_list:\n",
        "      subject_is_ancestor_verb = verb.is_ancestor(subject)\n",
        "      if(subject_is_ancestor_verb):\n",
        "        print(\"verb: {}, subject: {}\".format(verb,subject))\n",
        "\n",
        "  for object in objects_list:\n",
        "    for verb in verbs_list:\n",
        "      object_is_ancestor_verb = verb.is_ancestor(object)\n",
        "      if(object_is_ancestor_verb):\n",
        "        print(\"verb: {}, object: {}\".format(verb,object))\n",
        "\n",
        "  print(\"____________________________________________________\")"
      ],
      "metadata": {
        "id": "RfXpy6WD5JyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "doc = nlp(final_text)\n",
        "\n",
        "sentences_list = list(doc.sents)\n",
        "\n",
        "for sentence in sentences_list:\n",
        "  for token in sentence:\n",
        "    if(token.dep_ == \"nsubj\"):\n",
        "      print(token, list(token.ancestors))\n",
        "  print(\"____________________________________________________\")"
      ],
      "metadata": {
        "id": "VfPEN7dEsM8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "doc = nlp(final_text)\n",
        "\n",
        "sentences_list = list(doc.sents)\n",
        "\n",
        "for sentence in sentences_list:\n",
        "  for token in sentence:\n",
        "    if(token.dep_ == \"nsubj\"):\n",
        "      print(token, list(token.subtree))\n",
        "  print(\"____________________________________________________\")"
      ],
      "metadata": {
        "id": "FFBjoyP_uGqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "doc = nlp(final_text)\n",
        "\n",
        "sentences_list = list(doc.sents)\n",
        "\n",
        "for sentence in sentences_list:\n",
        "  for token in sentence:\n",
        "    if(token.dep_ == \"dobj\"):\n",
        "      print(token, list(token.ancestors))\n",
        "  print(\"____________________________________________________\")"
      ],
      "metadata": {
        "id": "ryyFqMGDrjpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "doc = nlp(final_text)\n",
        "\n",
        "sentences_list = list(doc.sents)\n",
        "\n",
        "for sentence in sentences_list:\n",
        "  for token in sentence:\n",
        "    if(token.dep_ == \"dobj\"):\n",
        "      print(token, list(token.subtree))\n",
        "  print(\"____________________________________________________\")"
      ],
      "metadata": {
        "id": "v_D8lZi2vAoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "doc = nlp(final_text)\n",
        "\n",
        "sentences_list = list(doc.sents)\n",
        "\n",
        "for sentence in sentences_list:\n",
        "  for token in sentence:\n",
        "    if(token.pos_ == \"VERB\"):\n",
        "      print(token, list(token.children))\n",
        "  print(\"____________________________________________________\")"
      ],
      "metadata": {
        "id": "e7cpLkoxsgwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "doc = nlp(final_text)\n",
        "\n",
        "sentences_list = list(doc.sents)\n",
        "\n",
        "for sentence in sentences_list:\n",
        "  for token in sentence:\n",
        "    if(token.pos_ == \"VERB\"):\n",
        "      print(token, list(token.subtree))\n",
        "  print(\"____________________________________________________\")"
      ],
      "metadata": {
        "id": "FQ0OiADzvNOT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}